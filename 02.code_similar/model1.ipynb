{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import itertools as it\n",
    "import functools as fn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('./dataset/data_df.csv')\n",
    "data_df = pd.read_csv(data_path)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(code):\n",
    "    code = re.sub(r'    ',r'\\t ',code)\n",
    "    code = re.sub(r\"#.*\\n\",'\\n',code)\n",
    "    code = re.sub(r'\"',r\"'\",code)\n",
    "    code = re.sub(r\"([\\n:(){}\\[\\]\\*\\/\\%\\+\\-\\,\\=.'])\",r' \\1 ',code)\n",
    "    code = re.sub(r'\\n',r\"<n>\",code)\n",
    "    code = re.sub(r'\\t',r\"<t>\",code)\n",
    "    return code.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import config as cfg\n",
    "\n",
    "with open(data_df.code1[0],'r') as f:\n",
    "    code1 = f.read()\n",
    "with open(data_df.code2[0],'r') as f:\n",
    "    code2 = f.read()\n",
    "    \n",
    "code_docs = [code1,code2]\n",
    "\n",
    "for i in range(len(code_docs)):\n",
    "    code_docs[i] = preprocessing(code_docs[i])\n",
    "\n",
    "print(f'{len(code_docs)=}')    \n",
    "# print(code_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "code_path = './dataset/code/'\n",
    "classes = os.listdir(code_path)\n",
    "source_list = []\n",
    "for label in classes:\n",
    "    filenames = os.listdir(code_path+label)\n",
    "    for file in filenames:\n",
    "        source_list.append(code_path+label+'/'+file)\n",
    "\n",
    "# source_list[145:155]\n",
    "len(source_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(source_list[478])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "code_docs = []\n",
    "for src_path in tqdm(source_list):\n",
    "    with open(src_path,'r',encoding='utf-8') as f:\n",
    "        code_docs.append(f.read())\n",
    "len(code_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max([len(code_doc) for code_doc in code_docs])\n",
    "print(f'{max_len=}')\n",
    "max_len_id = np.argmax([len(code_doc) for code_doc in code_docs])\n",
    "print(source_list[max_len_id])\n",
    "# print(code_docs[max_len_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_code = max(code_docs)\n",
    "print(\"code_docs.index(max_id)\")\n",
    "print(code_docs.index(max_code))\n",
    "print()\n",
    "print(\"source_list[code_docs.index(max_id)]\")\n",
    "print(source_list[code_docs.index(max_code)])\n",
    "print()\n",
    "# print(max_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import config as cfg\n",
    "\n",
    "for i in tqdm(range(len(code_docs))):\n",
    "    code_docs[i] = preprocessing(code_docs[i]).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max([len(code_doc) for code_doc in code_docs])\n",
    "print(f'{max_len=}')\n",
    "max_len_id = np.argmax([len(code_doc) for code_doc in code_docs])\n",
    "print(source_list[max_len_id])\n",
    "# print(code_docs[max_len_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_code = max(code_docs)\n",
    "print(\"code_docs.index(max_id)\")\n",
    "print(code_docs.index(max_code))\n",
    "print()\n",
    "print(\"source_list[code_docs.index(max_id)]\")\n",
    "print(source_list[code_docs.index(max_code)])\n",
    "print()\n",
    "# print(max_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "for code_doc in tqdm(code_docs):\n",
    "    # code_doc = np.concatenate(code_doc).tolist() # 메모리 부족....\n",
    "    counter += Counter(code_doc)\n",
    "most_counter = counter.most_common(10000-2)\n",
    "vocab = ['<pad>','<unk>']+[key for key, _ in most_counter]\n",
    "# print(vocab)\n",
    "word_to_index = {word:index for index, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word_to_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input pipe line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "del code_docs\n",
    "del most_counter\n",
    "del source_list\n",
    "del counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(path):\n",
    "    with open(path,'r',encoding='utf8') as f:\n",
    "        code = f.read()\n",
    "    words = preprocessing(code)\n",
    "    code_sequence = [word_to_index[word] if word in word_to_index else word_to_index['<unk>'] for word in words]\n",
    "    return code_sequence\n",
    "\n",
    "def make_tensor(paths):\n",
    "    code_sequences = []\n",
    "    for path in tqdm(paths):\n",
    "        code_sequences.append(tokenize(path))\n",
    "    pad_sequences = keras.preprocessing.sequence.pad_sequences(code_sequences,maxlen=cfg.max_len,truncating='pre')\n",
    "    return pad_sequences\n",
    "    \n",
    "def make_dataset(code1_paths,code2_paths,similar):\n",
    "    code1_tensor = make_tensor(code1_paths)\n",
    "    code1_ds = tf.data.Dataset.from_tensor_slices(code1_tensor)\n",
    "    code2_tensor = make_tensor(code2_paths)\n",
    "    code2_ds = tf.data.Dataset.from_tensor_slices(code2_tensor)\n",
    "    similar_ds = tf.data.Dataset.from_tensor_slices(similar)\n",
    "    ds = tf.data.Dataset.zip((code1_ds,code2_ds,similar_ds))\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ds = make_dataset(data_df['code1'],data_df['code2'],data_df['similar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512,)\n",
      "(512,)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for code1, code2, similar in data_ds.take(1):\n",
    "    print(code1.shape)\n",
    "    print(code2.shape)\n",
    "    print(similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9996"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train_ds=7996\n",
      "num_val_ds=2000\n"
     ]
    }
   ],
   "source": [
    "num_train_ds = int(len(data_ds)*0.8)\n",
    "print(f\"{num_train_ds=}\")\n",
    "num_val_ds = len(data_ds)-num_train_ds\n",
    "print(f\"{num_val_ds=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = data_ds.take(num_train_ds)\n",
    "val_ds = data_ds.skip(num_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_ds)=7996\n",
      "len(val_ds)=2000\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(train_ds)=}\")\n",
    "print(f\"{len(val_ds)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 셀프 어텐션\n",
    "d_model = 512\n",
    "num_layers = 6\n",
    "num_heads = 8\n",
    "dff = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 포지셔널 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2*(i//2))/np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, num_heads=8):\n",
    "\n",
    "        assert embedding_dim % num_heads == 0,\"embedding_dim % num_heads != 0\"\n",
    "\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.projection_dim\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d102a36440b3fa6f6e9070927b3490df4f74782c023ea09ceab8356b50f0a087"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
