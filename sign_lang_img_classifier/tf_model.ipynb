{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4642be84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35704221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5783409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6878276661807658806\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-25 18:22:46.244771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-05-25 18:22:46.247449: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af512893",
   "metadata": {},
   "source": [
    "# tf input pipline\n",
    "## tf.data.dataset 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2019b503",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"./dataset/main_directory/\"\n",
    "test_path = \"./dataset/test/\"\n",
    "\n",
    "INPUT_SHAPE = (224,224,3)\n",
    "BATCH_SIZE = 32\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "tf.random.set_seed(7)\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0779fdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_image(filename):\n",
    "    img = tf.io.read_file(filename)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.resize(img,INPUT_SHAPE[:2])\n",
    "    return img\n",
    "\n",
    "def make_dataset(filepaths,labels):\n",
    "    filenames_ds = tf.data.Dataset.from_tensor_slices(filepaths)\n",
    "    images_ds = filenames_ds.map(\n",
    "        parse_image,\n",
    "        num_parallel_calls=AUTOTUNE\n",
    "    )\n",
    "    labels_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    ds = tf.data.Dataset.zip((images_ds, labels_ds))\n",
    "    return ds\n",
    "\n",
    "def configure_for_performance(ds):\n",
    "    ds = ds.shuffle(buffer_size=1000)\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    ds = ds.repeat()\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e61416",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = os.listdir(train_path)\n",
    "num_classes = len(classes)\n",
    "print(\"number of classes :\", num_classes)\n",
    "\n",
    "filenames = glob(train_path+'*/*')\n",
    "num_images = len(filenames)\n",
    "print(\"number of images :\", num_images)\n",
    "\n",
    "print(filenames[0])\n",
    "print(filenames[0].split(os.sep)[-2])\n",
    "print()\n",
    "\n",
    "np.random.shuffle(filenames)\n",
    "labels = [classes.index(fn.split(os.sep)[-2]) for fn in filenames]\n",
    "\n",
    "print(\"data 5 preview\")\n",
    "for path,label in zip(filenames[:5],labels[:5]):\n",
    "    print(path)\n",
    "    print(\"class :\",classes[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52580d6a",
   "metadata": {},
   "source": [
    "# 학습셋, 검증셋으로 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aaea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, val_x, train_y, val_y = train_test_split(\n",
    "    filenames, labels, test_size=0.2,\n",
    "    stratify=labels, random_state=1\n",
    ")\n",
    "num_train = len(train_x)\n",
    "num_val = len(val_x)\n",
    "print(\"number of training data :\", num_train)\n",
    "print(\"number of validation data :\", num_val)\n",
    "\n",
    "print(\"data 5 preview\")\n",
    "for path,label in zip(train_x[:5],train_y[:5]):\n",
    "    print(path)\n",
    "    print(\"class :\",classes[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad436c5",
   "metadata": {},
   "source": [
    "# tf dataset 객체 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a1f9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = make_dataset(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ecf94b",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# tf.data.dataset 확인\n",
    "\n",
    "for img, label in train_ds.take(5):\n",
    "    # print(img.numpy().shape)\n",
    "    img = img.numpy()\n",
    "    # print(img.min(), img.max())\n",
    "    \n",
    "    img = img.astype(np.uint8)\n",
    "    idx = (label.numpy())\n",
    "    \n",
    "    plt.imshow(img), plt.axis('off')\n",
    "    plt.title(classes[idx])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e764d72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = make_dataset(val_x, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3e5691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 성능 높이기\n",
    "train_ds = configure_for_performance(train_ds)\n",
    "val_ds = configure_for_performance(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1974c928",
   "metadata": {},
   "source": [
    "tf.data.experimental.save(train_ds, \"train_ds\", compression=\"GZIP\")\n",
    "tf.data.experimental.save(val_ds, \"val_ds\", compression=\"GZIP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6d4c0b",
   "metadata": {},
   "source": [
    "train_ds = tf.data.experimental.load(\"train_ds\", compression=\"GZIP\")\n",
    "val_ds = tf.data.experimental.load(\"val_ds\", compression=\"GZIP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b12369",
   "metadata": {},
   "source": [
    "# data augmentation layer\n",
    "## evaluate() 또는 predict() 호출 시에는 자동으로 비활성화 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee8c6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential([\n",
    "    keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "    keras.layers.experimental.preprocessing.RandomTranslation(0.1,0.1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc848d0",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103a8d62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transfer_model = keras.applications.Xception(\n",
    "    input_shape= INPUT_SHAPE,\n",
    "    include_top= False,\n",
    "    weights= 'imagenet',\n",
    ")\n",
    "transfer_model.trainable = True\n",
    "# transfer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac2f51f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "finetune_model = keras.Sequential([\n",
    "    keras.Input(shape=INPUT_SHAPE),\n",
    "    data_augmentation,\n",
    "    keras.layers.experimental.preprocessing.Rescaling(1./255),\n",
    "    transfer_model,\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(2000, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(1000, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "# finetune_model.summary()\n",
    "\n",
    "finetune_model.compile(loss='sparse_categorical_crossentropy',\n",
    "                      optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "                       metrics='accuracy')                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d5fbc2",
   "metadata": {},
   "source": [
    "# batch 데이터셋 확인\n",
    "\n",
    "for batch in train_ds.take(1):\n",
    "    for i in range(5):\n",
    "        img = batch[0][i]\n",
    "        # print(img.numpy().shape)\n",
    "        img = img.numpy()\n",
    "        # print(img.min(), img.max())\n",
    "        \n",
    "        img = img.astype(np.uint8)\n",
    "        \n",
    "        label = batch[1][i]\n",
    "        idx = (label.numpy())\n",
    "        \n",
    "        plt.imshow(img), plt.axis('off')\n",
    "        plt.title(classes[idx])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3499551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = num_train//BATCH_SIZE\n",
    "val_step = num_val//BATCH_SIZE\n",
    "\n",
    "filepath = \"./model/{epoch:03d}-{val_loss:.4f}.h5\"\n",
    "check_point = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=filepath,\n",
    "    save_best_only = True,\n",
    "    verbose = True\n",
    ")\n",
    "early_stop_point = keras.callbacks.EarlyStopping(patience=50)\n",
    "\n",
    "hist = finetune_model.fit(\n",
    "    train_ds, epochs=1000,\n",
    "    steps_per_epoch=train_step,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps=val_step,\n",
    "    callbacks = [check_point, early_stop_point]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d9dda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history[\"loss\"], c='b', label=\"train_loss\")\n",
    "plt.plot(hist.history[\"val_loss\"], c='r', label=\"val loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7b6829",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a2c4b191d1ae843dde5cb5f4d1f62fa892f6b79b0f9392a84691e890e33c5a4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
