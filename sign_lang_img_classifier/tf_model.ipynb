{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4642be84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af512893",
   "metadata": {},
   "source": [
    "# tf input pipline\n",
    "## tf.data.dataset 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2019b503",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"./dataset/main_directory/\"\n",
    "test_path = \"./dataset/test/\"\n",
    "\n",
    "INPUT_SHAPE = (224,224,3)\n",
    "BATCH_SIZE = 32\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "tf.random.set_seed(7)\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0779fdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_image(filename):\n",
    "    img = tf.io.read_file(filename)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.resize(img,INPUT_SHAPE[:2])\n",
    "    return img\n",
    "\n",
    "def make_dataset(filepaths,labels):\n",
    "    filenames_ds = tf.data.Dataset.from_tensor_slices(filepaths)\n",
    "    images_ds = filenames_ds.map(\n",
    "        parse_image,\n",
    "        num_parallel_calls=AUTOTUNE\n",
    "    )\n",
    "    labels_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    ds = tf.data.Dataset.zip((images_ds, labels_ds))\n",
    "    return ds\n",
    "\n",
    "def configure_for_performance(ds):\n",
    "    ds = ds.shuffle(buffer_size=1000)\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    ds = ds.repeat()\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29e61416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of classes : 11\n",
      "number of images : 858\n",
      "./dataset/main_directory\\1\\1_train_031.png\n",
      "1\n",
      "\n",
      "data 5 preview\n",
      "./dataset/main_directory\\5\\5_train_617.png\n",
      "class : 5\n",
      "./dataset/main_directory\\8\\8_train_838.png\n",
      "class : 8\n",
      "./dataset/main_directory\\9\\9_train_387.png\n",
      "class : 9\n",
      "./dataset/main_directory\\8\\8_train_007.png\n",
      "class : 8\n",
      "./dataset/main_directory\\4\\4_train_504.png\n",
      "class : 4\n"
     ]
    }
   ],
   "source": [
    "classes = os.listdir(train_path)\n",
    "num_classes = len(classes)\n",
    "print(\"number of classes :\", num_classes)\n",
    "\n",
    "filenames = glob(train_path+'*/*')\n",
    "num_images = len(filenames)\n",
    "print(\"number of images :\", num_images)\n",
    "\n",
    "print(filenames[0])\n",
    "print(filenames[0].split(os.sep)[-2])\n",
    "print()\n",
    "\n",
    "np.random.shuffle(filenames)\n",
    "labels = [classes.index(fn.split(os.sep)[-2]) for fn in filenames]\n",
    "\n",
    "print(\"data 5 preview\")\n",
    "for path,label in zip(filenames[:5],labels[:5]):\n",
    "    print(path)\n",
    "    print(\"class :\",classes[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52580d6a",
   "metadata": {},
   "source": [
    "# 학습셋, 검증셋으로 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89aaea30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training data : 686\n",
      "number of validation data : 172\n",
      "data 5 preview\n",
      "./dataset/main_directory\\8\\8_train_703.png\n",
      "class : 8\n",
      "./dataset/main_directory\\5\\5_train_442.png\n",
      "class : 5\n",
      "./dataset/main_directory\\5\\5_train_611.png\n",
      "class : 5\n",
      "./dataset/main_directory\\4\\4_train_796.png\n",
      "class : 4\n",
      "./dataset/main_directory\\10-1\\10-1_train_390.png\n",
      "class : 10-1\n"
     ]
    }
   ],
   "source": [
    "train_x, val_x, train_y, val_y = train_test_split(\n",
    "    filenames, labels, test_size=0.2,\n",
    "    stratify=labels, random_state=1\n",
    ")\n",
    "num_train = len(train_x)\n",
    "num_val = len(val_x)\n",
    "print(\"number of training data :\", num_train)\n",
    "print(\"number of validation data :\", num_val)\n",
    "\n",
    "print(\"data 5 preview\")\n",
    "for path,label in zip(train_x[:5],train_y[:5]):\n",
    "    print(path)\n",
    "    print(\"class :\",classes[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad436c5",
   "metadata": {},
   "source": [
    "# tf dataset 객체 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28a1f9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = make_dataset(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ecf94b",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# tf.data.dataset 확인\n",
    "\n",
    "for img, label in train_ds.take(5):\n",
    "    # print(img.numpy().shape)\n",
    "    img = img.numpy()\n",
    "    # print(img.min(), img.max())\n",
    "    \n",
    "    img = img.astype(np.uint8)\n",
    "    idx = (label.numpy())\n",
    "    \n",
    "    plt.imshow(img), plt.axis('off')\n",
    "    plt.title(classes[idx])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e764d72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = make_dataset(val_x, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a3e5691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 성능 높이기\n",
    "train_ds = configure_for_performance(train_ds)\n",
    "val_ds = configure_for_performance(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1974c928",
   "metadata": {},
   "source": [
    "tf.data.experimental.save(train_ds, \"train_ds\", compression=\"GZIP\")\n",
    "tf.data.experimental.save(val_ds, \"val_ds\", compression=\"GZIP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6d4c0b",
   "metadata": {},
   "source": [
    "train_ds = tf.data.experimental.load(\"train_ds\", compression=\"GZIP\")\n",
    "val_ds = tf.data.experimental.load(\"val_ds\", compression=\"GZIP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b12369",
   "metadata": {},
   "source": [
    "# data augmentation layer\n",
    "## evaluate() 또는 predict() 호출 시에는 자동으로 비활성화 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ee8c6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential([\n",
    "    keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "    keras.layers.experimental.preprocessing.RandomTranslation(0.1,0.1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc848d0",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "103a8d62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transfer_model = keras.applications.Xception(\n",
    "    input_shape= INPUT_SHAPE,\n",
    "    include_top= False,\n",
    "    weights= 'imagenet',\n",
    ")\n",
    "transfer_model.trainable = False\n",
    "# transfer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ac2f51f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "finetune_model = keras.Sequential([\n",
    "    keras.Input(shape=INPUT_SHAPE),\n",
    "    data_augmentation,\n",
    "    keras.layers.experimental.preprocessing.Rescaling(1./255),\n",
    "    transfer_model,\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(2000, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(1000, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(500, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "# finetune_model.summary()\n",
    "\n",
    "finetune_model.compile(loss='sparse_categorical_crossentropy',\n",
    "                      optimizer=keras.optimizers.Adam(learning_rate=0.0002),\n",
    "                       metrics='accuracy')                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d5fbc2",
   "metadata": {},
   "source": [
    "# batch 데이터셋 확인\n",
    "\n",
    "for batch in train_ds.take(1):\n",
    "    for i in range(5):\n",
    "        img = batch[0][i]\n",
    "        # print(img.numpy().shape)\n",
    "        img = img.numpy()\n",
    "        # print(img.min(), img.max())\n",
    "        \n",
    "        img = img.astype(np.uint8)\n",
    "        \n",
    "        label = batch[1][i]\n",
    "        idx = (label.numpy())\n",
    "        \n",
    "        plt.imshow(img), plt.axis('off')\n",
    "        plt.title(classes[idx])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3499551b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 3.3380 - accuracy: 0.1280\n",
      "Epoch 1: val_loss improved from inf to 2.11878, saving model to ./model\\001-2.1188.h5\n",
      "21/21 [==============================] - 133s 6s/step - loss: 3.3380 - accuracy: 0.1280 - val_loss: 2.1188 - val_accuracy: 0.2313\n",
      "Epoch 2/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 2.5001 - accuracy: 0.2125\n",
      "Epoch 2: val_loss improved from 2.11878 to 2.00672, saving model to ./model\\002-2.0067.h5\n",
      "21/21 [==============================] - 117s 6s/step - loss: 2.5001 - accuracy: 0.2125 - val_loss: 2.0067 - val_accuracy: 0.3250\n",
      "Epoch 3/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 2.2829 - accuracy: 0.2370\n",
      "Epoch 3: val_loss improved from 2.00672 to 1.76597, saving model to ./model\\003-1.7660.h5\n",
      "21/21 [==============================] - 119s 6s/step - loss: 2.2829 - accuracy: 0.2370 - val_loss: 1.7660 - val_accuracy: 0.4062\n",
      "Epoch 4/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 2.0222 - accuracy: 0.3119\n",
      "Epoch 4: val_loss improved from 1.76597 to 1.62146, saving model to ./model\\004-1.6215.h5\n",
      "21/21 [==============================] - 115s 6s/step - loss: 2.0222 - accuracy: 0.3119 - val_loss: 1.6215 - val_accuracy: 0.4563\n",
      "Epoch 5/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.8384 - accuracy: 0.3394\n",
      "Epoch 5: val_loss improved from 1.62146 to 1.44224, saving model to ./model\\005-1.4422.h5\n",
      "21/21 [==============================] - 113s 5s/step - loss: 1.8384 - accuracy: 0.3394 - val_loss: 1.4422 - val_accuracy: 0.4938\n",
      "Epoch 6/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.7119 - accuracy: 0.4052\n",
      "Epoch 6: val_loss improved from 1.44224 to 1.38465, saving model to ./model\\006-1.3846.h5\n",
      "21/21 [==============================] - 113s 5s/step - loss: 1.7119 - accuracy: 0.4052 - val_loss: 1.3846 - val_accuracy: 0.5125\n",
      "Epoch 7/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5241 - accuracy: 0.4526\n",
      "Epoch 7: val_loss improved from 1.38465 to 1.22626, saving model to ./model\\007-1.2263.h5\n",
      "21/21 [==============================] - 114s 5s/step - loss: 1.5241 - accuracy: 0.4526 - val_loss: 1.2263 - val_accuracy: 0.5312\n",
      "Epoch 8/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.4325 - accuracy: 0.4893\n",
      "Epoch 8: val_loss improved from 1.22626 to 1.11325, saving model to ./model\\008-1.1133.h5\n",
      "21/21 [==============================] - 113s 5s/step - loss: 1.4325 - accuracy: 0.4893 - val_loss: 1.1133 - val_accuracy: 0.5813\n",
      "Epoch 9/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.3957 - accuracy: 0.5138\n",
      "Epoch 9: val_loss improved from 1.11325 to 1.04640, saving model to ./model\\009-1.0464.h5\n",
      "21/21 [==============================] - 120s 6s/step - loss: 1.3957 - accuracy: 0.5138 - val_loss: 1.0464 - val_accuracy: 0.5938\n",
      "Epoch 10/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2850 - accuracy: 0.5489\n",
      "Epoch 10: val_loss improved from 1.04640 to 0.92566, saving model to ./model\\010-0.9257.h5\n",
      "21/21 [==============================] - 118s 6s/step - loss: 1.2850 - accuracy: 0.5489 - val_loss: 0.9257 - val_accuracy: 0.6500\n",
      "Epoch 11/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0579 - accuracy: 0.6300\n",
      "Epoch 11: val_loss improved from 0.92566 to 0.91197, saving model to ./model\\011-0.9120.h5\n",
      "21/21 [==============================] - 112s 5s/step - loss: 1.0579 - accuracy: 0.6300 - val_loss: 0.9120 - val_accuracy: 0.6750\n",
      "Epoch 12/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0259 - accuracy: 0.6315\n",
      "Epoch 12: val_loss improved from 0.91197 to 0.79346, saving model to ./model\\012-0.7935.h5\n",
      "21/21 [==============================] - 115s 6s/step - loss: 1.0259 - accuracy: 0.6315 - val_loss: 0.7935 - val_accuracy: 0.7125\n",
      "Epoch 13/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9910 - accuracy: 0.6407\n",
      "Epoch 13: val_loss did not improve from 0.79346\n",
      "21/21 [==============================] - 110s 5s/step - loss: 0.9910 - accuracy: 0.6407 - val_loss: 0.8785 - val_accuracy: 0.6375\n",
      "Epoch 14/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9433 - accuracy: 0.6560\n",
      "Epoch 14: val_loss improved from 0.79346 to 0.75784, saving model to ./model\\014-0.7578.h5\n",
      "21/21 [==============================] - 111s 5s/step - loss: 0.9433 - accuracy: 0.6560 - val_loss: 0.7578 - val_accuracy: 0.7250\n",
      "Epoch 15/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9023 - accuracy: 0.6606\n",
      "Epoch 15: val_loss improved from 0.75784 to 0.69397, saving model to ./model\\015-0.6940.h5\n",
      "21/21 [==============================] - 112s 5s/step - loss: 0.9023 - accuracy: 0.6606 - val_loss: 0.6940 - val_accuracy: 0.7437\n",
      "Epoch 16/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8187 - accuracy: 0.6758\n",
      "Epoch 16: val_loss did not improve from 0.69397\n",
      "21/21 [==============================] - 106s 5s/step - loss: 0.8187 - accuracy: 0.6758 - val_loss: 0.7629 - val_accuracy: 0.6875\n",
      "Epoch 17/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7654 - accuracy: 0.7080\n",
      "Epoch 17: val_loss improved from 0.69397 to 0.69113, saving model to ./model\\017-0.6911.h5\n",
      "21/21 [==============================] - 111s 5s/step - loss: 0.7654 - accuracy: 0.7080 - val_loss: 0.6911 - val_accuracy: 0.7563\n",
      "Epoch 18/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6860 - accuracy: 0.7385\n",
      "Epoch 18: val_loss improved from 0.69113 to 0.63256, saving model to ./model\\018-0.6326.h5\n",
      "21/21 [==============================] - 111s 5s/step - loss: 0.6860 - accuracy: 0.7385 - val_loss: 0.6326 - val_accuracy: 0.7688\n",
      "Epoch 19/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7042 - accuracy: 0.7492\n",
      "Epoch 19: val_loss did not improve from 0.63256\n",
      "21/21 [==============================] - 105s 5s/step - loss: 0.7042 - accuracy: 0.7492 - val_loss: 0.6740 - val_accuracy: 0.7500\n",
      "Epoch 20/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6553 - accuracy: 0.7554\n",
      "Epoch 20: val_loss did not improve from 0.63256\n",
      "21/21 [==============================] - 106s 5s/step - loss: 0.6553 - accuracy: 0.7554 - val_loss: 0.6787 - val_accuracy: 0.7437\n",
      "Epoch 21/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6229 - accuracy: 0.7615\n",
      "Epoch 21: val_loss did not improve from 0.63256\n",
      "21/21 [==============================] - 107s 5s/step - loss: 0.6229 - accuracy: 0.7615 - val_loss: 0.6392 - val_accuracy: 0.7312\n",
      "Epoch 22/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6246 - accuracy: 0.7859\n",
      "Epoch 22: val_loss improved from 0.63256 to 0.59403, saving model to ./model\\022-0.5940.h5\n",
      "21/21 [==============================] - 112s 5s/step - loss: 0.6246 - accuracy: 0.7859 - val_loss: 0.5940 - val_accuracy: 0.8188\n",
      "Epoch 23/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6617 - accuracy: 0.7634\n",
      "Epoch 23: val_loss did not improve from 0.59403\n",
      "21/21 [==============================] - 108s 5s/step - loss: 0.6617 - accuracy: 0.7634 - val_loss: 0.6995 - val_accuracy: 0.7625\n",
      "Epoch 24/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6203 - accuracy: 0.7661\n",
      "Epoch 24: val_loss improved from 0.59403 to 0.56509, saving model to ./model\\024-0.5651.h5\n",
      "21/21 [==============================] - 110s 5s/step - loss: 0.6203 - accuracy: 0.7661 - val_loss: 0.5651 - val_accuracy: 0.8188\n",
      "Epoch 25/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4720 - accuracy: 0.8196\n",
      "Epoch 25: val_loss did not improve from 0.56509\n",
      "21/21 [==============================] - 107s 5s/step - loss: 0.4720 - accuracy: 0.8196 - val_loss: 0.5829 - val_accuracy: 0.7812\n",
      "Epoch 26/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4902 - accuracy: 0.8043\n",
      "Epoch 26: val_loss did not improve from 0.56509\n",
      "21/21 [==============================] - 110s 5s/step - loss: 0.4902 - accuracy: 0.8043 - val_loss: 0.5885 - val_accuracy: 0.8125\n",
      "Epoch 27/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5080 - accuracy: 0.8104\n",
      "Epoch 27: val_loss did not improve from 0.56509\n",
      "21/21 [==============================] - 99s 5s/step - loss: 0.5080 - accuracy: 0.8104 - val_loss: 0.6050 - val_accuracy: 0.8188\n",
      "Epoch 28/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4256 - accuracy: 0.8502\n",
      "Epoch 28: val_loss improved from 0.56509 to 0.49283, saving model to ./model\\028-0.4928.h5\n",
      "21/21 [==============================] - 101s 5s/step - loss: 0.4256 - accuracy: 0.8502 - val_loss: 0.4928 - val_accuracy: 0.8062\n",
      "Epoch 29/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4099 - accuracy: 0.8593\n",
      "Epoch 29: val_loss did not improve from 0.49283\n",
      "21/21 [==============================] - 97s 5s/step - loss: 0.4099 - accuracy: 0.8593 - val_loss: 0.5456 - val_accuracy: 0.7688\n",
      "Epoch 30/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4748 - accuracy: 0.8272\n",
      "Epoch 30: val_loss did not improve from 0.49283\n",
      "21/21 [==============================] - 100s 5s/step - loss: 0.4748 - accuracy: 0.8272 - val_loss: 0.5803 - val_accuracy: 0.8062\n",
      "Epoch 31/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3505 - accuracy: 0.8685\n",
      "Epoch 31: val_loss improved from 0.49283 to 0.48791, saving model to ./model\\031-0.4879.h5\n",
      "21/21 [==============================] - 108s 5s/step - loss: 0.3505 - accuracy: 0.8685 - val_loss: 0.4879 - val_accuracy: 0.8313\n",
      "Epoch 32/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3561 - accuracy: 0.8792\n",
      "Epoch 32: val_loss improved from 0.48791 to 0.45454, saving model to ./model\\032-0.4545.h5\n",
      "21/21 [==============================] - 107s 5s/step - loss: 0.3561 - accuracy: 0.8792 - val_loss: 0.4545 - val_accuracy: 0.8500\n",
      "Epoch 33/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3372 - accuracy: 0.8716\n",
      "Epoch 33: val_loss did not improve from 0.45454\n",
      "21/21 [==============================] - 95s 5s/step - loss: 0.3372 - accuracy: 0.8716 - val_loss: 0.5492 - val_accuracy: 0.8313\n",
      "Epoch 34/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3791 - accuracy: 0.8761\n",
      "Epoch 34: val_loss did not improve from 0.45454\n",
      "21/21 [==============================] - 97s 5s/step - loss: 0.3791 - accuracy: 0.8761 - val_loss: 0.5840 - val_accuracy: 0.8250\n",
      "Epoch 35/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3604 - accuracy: 0.8670\n",
      "Epoch 35: val_loss did not improve from 0.45454\n",
      "21/21 [==============================] - 95s 5s/step - loss: 0.3604 - accuracy: 0.8670 - val_loss: 0.4856 - val_accuracy: 0.8250\n",
      "Epoch 36/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3015 - accuracy: 0.8930\n",
      "Epoch 36: val_loss did not improve from 0.45454\n",
      "21/21 [==============================] - 93s 4s/step - loss: 0.3015 - accuracy: 0.8930 - val_loss: 0.4769 - val_accuracy: 0.8687\n",
      "Epoch 37/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3505 - accuracy: 0.8930\n",
      "Epoch 37: val_loss did not improve from 0.45454\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.3505 - accuracy: 0.8930 - val_loss: 0.5033 - val_accuracy: 0.8313\n",
      "Epoch 38/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3371 - accuracy: 0.8823\n",
      "Epoch 38: val_loss did not improve from 0.45454\n",
      "21/21 [==============================] - 103s 5s/step - loss: 0.3371 - accuracy: 0.8823 - val_loss: 0.4950 - val_accuracy: 0.8375\n",
      "Epoch 39/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2894 - accuracy: 0.8899\n",
      "Epoch 39: val_loss improved from 0.45454 to 0.43891, saving model to ./model\\039-0.4389.h5\n",
      "21/21 [==============================] - 111s 5s/step - loss: 0.2894 - accuracy: 0.8899 - val_loss: 0.4389 - val_accuracy: 0.8438\n",
      "Epoch 40/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3065 - accuracy: 0.8976\n",
      "Epoch 40: val_loss did not improve from 0.43891\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.3065 - accuracy: 0.8976 - val_loss: 0.4770 - val_accuracy: 0.8562\n",
      "Epoch 41/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2749 - accuracy: 0.9159\n",
      "Epoch 41: val_loss did not improve from 0.43891\n",
      "21/21 [==============================] - 99s 5s/step - loss: 0.2749 - accuracy: 0.9159 - val_loss: 0.4715 - val_accuracy: 0.8625\n",
      "Epoch 42/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2883 - accuracy: 0.9098\n",
      "Epoch 42: val_loss did not improve from 0.43891\n",
      "21/21 [==============================] - 100s 5s/step - loss: 0.2883 - accuracy: 0.9098 - val_loss: 0.4467 - val_accuracy: 0.8438\n",
      "Epoch 43/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2808 - accuracy: 0.9144\n",
      "Epoch 43: val_loss did not improve from 0.43891\n",
      "21/21 [==============================] - 95s 5s/step - loss: 0.2808 - accuracy: 0.9144 - val_loss: 0.4556 - val_accuracy: 0.8188\n",
      "Epoch 44/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2578 - accuracy: 0.9083\n",
      "Epoch 44: val_loss did not improve from 0.43891\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.2578 - accuracy: 0.9083 - val_loss: 0.5132 - val_accuracy: 0.8062\n",
      "Epoch 45/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2132 - accuracy: 0.9226\n",
      "Epoch 45: val_loss improved from 0.43891 to 0.41056, saving model to ./model\\045-0.4106.h5\n",
      "21/21 [==============================] - 105s 5s/step - loss: 0.2132 - accuracy: 0.9226 - val_loss: 0.4106 - val_accuracy: 0.8750\n",
      "Epoch 46/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2530 - accuracy: 0.9159\n",
      "Epoch 46: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 94s 5s/step - loss: 0.2530 - accuracy: 0.9159 - val_loss: 0.5560 - val_accuracy: 0.8625\n",
      "Epoch 47/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3003 - accuracy: 0.8945\n",
      "Epoch 47: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 107s 5s/step - loss: 0.3003 - accuracy: 0.8945 - val_loss: 0.5612 - val_accuracy: 0.8625\n",
      "Epoch 48/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2182 - accuracy: 0.9281\n",
      "Epoch 48: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 106s 5s/step - loss: 0.2182 - accuracy: 0.9281 - val_loss: 0.5132 - val_accuracy: 0.8500\n",
      "Epoch 49/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2447 - accuracy: 0.9220\n",
      "Epoch 49: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.2447 - accuracy: 0.9220 - val_loss: 0.5502 - val_accuracy: 0.8375\n",
      "Epoch 50/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3367 - accuracy: 0.8976\n",
      "Epoch 50: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 97s 5s/step - loss: 0.3367 - accuracy: 0.8976 - val_loss: 0.5147 - val_accuracy: 0.8375\n",
      "Epoch 51/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3219 - accuracy: 0.9006\n",
      "Epoch 51: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 103s 5s/step - loss: 0.3219 - accuracy: 0.9006 - val_loss: 0.5696 - val_accuracy: 0.8500\n",
      "Epoch 52/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2815 - accuracy: 0.8945\n",
      "Epoch 52: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 107s 5s/step - loss: 0.2815 - accuracy: 0.8945 - val_loss: 0.4258 - val_accuracy: 0.8750\n",
      "Epoch 53/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2076 - accuracy: 0.9159\n",
      "Epoch 53: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 109s 5s/step - loss: 0.2076 - accuracy: 0.9159 - val_loss: 0.5559 - val_accuracy: 0.8875\n",
      "Epoch 54/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2912 - accuracy: 0.9083\n",
      "Epoch 54: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 101s 5s/step - loss: 0.2912 - accuracy: 0.9083 - val_loss: 0.5559 - val_accuracy: 0.8750\n",
      "Epoch 55/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2641 - accuracy: 0.9235\n",
      "Epoch 55: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 101s 5s/step - loss: 0.2641 - accuracy: 0.9235 - val_loss: 0.4961 - val_accuracy: 0.8500\n",
      "Epoch 56/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1620 - accuracy: 0.9343\n",
      "Epoch 56: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 100s 5s/step - loss: 0.1620 - accuracy: 0.9343 - val_loss: 0.5779 - val_accuracy: 0.8375\n",
      "Epoch 57/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2431 - accuracy: 0.9266\n",
      "Epoch 57: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 99s 5s/step - loss: 0.2431 - accuracy: 0.9266 - val_loss: 0.4544 - val_accuracy: 0.8750\n",
      "Epoch 58/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1879 - accuracy: 0.9388\n",
      "Epoch 58: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 100s 5s/step - loss: 0.1879 - accuracy: 0.9388 - val_loss: 0.5038 - val_accuracy: 0.8750\n",
      "Epoch 59/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2057 - accuracy: 0.9327\n",
      "Epoch 59: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 99s 5s/step - loss: 0.2057 - accuracy: 0.9327 - val_loss: 0.5409 - val_accuracy: 0.8562\n",
      "Epoch 60/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1877 - accuracy: 0.9312\n",
      "Epoch 60: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 99s 5s/step - loss: 0.1877 - accuracy: 0.9312 - val_loss: 0.5114 - val_accuracy: 0.8687\n",
      "Epoch 61/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1719 - accuracy: 0.9358\n",
      "Epoch 61: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 100s 5s/step - loss: 0.1719 - accuracy: 0.9358 - val_loss: 0.4858 - val_accuracy: 0.8625\n",
      "Epoch 62/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1437 - accuracy: 0.9511\n",
      "Epoch 62: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 100s 5s/step - loss: 0.1437 - accuracy: 0.9511 - val_loss: 0.5775 - val_accuracy: 0.8562\n",
      "Epoch 63/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1767 - accuracy: 0.9327\n",
      "Epoch 63: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 102s 5s/step - loss: 0.1767 - accuracy: 0.9327 - val_loss: 0.4514 - val_accuracy: 0.8750\n",
      "Epoch 64/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1551 - accuracy: 0.9388\n",
      "Epoch 64: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 103s 5s/step - loss: 0.1551 - accuracy: 0.9388 - val_loss: 0.5370 - val_accuracy: 0.8562\n",
      "Epoch 65/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2349 - accuracy: 0.9144\n",
      "Epoch 65: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 100s 5s/step - loss: 0.2349 - accuracy: 0.9144 - val_loss: 0.5637 - val_accuracy: 0.8625\n",
      "Epoch 66/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1444 - accuracy: 0.9419\n",
      "Epoch 66: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 100s 5s/step - loss: 0.1444 - accuracy: 0.9419 - val_loss: 0.6124 - val_accuracy: 0.8250\n",
      "Epoch 67/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1892 - accuracy: 0.9330\n",
      "Epoch 67: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 105s 5s/step - loss: 0.1892 - accuracy: 0.9330 - val_loss: 0.4696 - val_accuracy: 0.8750\n",
      "Epoch 68/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1509 - accuracy: 0.9495\n",
      "Epoch 68: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 104s 5s/step - loss: 0.1509 - accuracy: 0.9495 - val_loss: 0.4489 - val_accuracy: 0.8938\n",
      "Epoch 69/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1341 - accuracy: 0.9526\n",
      "Epoch 69: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 103s 5s/step - loss: 0.1341 - accuracy: 0.9526 - val_loss: 0.5767 - val_accuracy: 0.8625\n",
      "Epoch 70/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1593 - accuracy: 0.9358\n",
      "Epoch 70: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 102s 5s/step - loss: 0.1593 - accuracy: 0.9358 - val_loss: 0.7044 - val_accuracy: 0.8375\n",
      "Epoch 71/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1862 - accuracy: 0.9404\n",
      "Epoch 71: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 107s 5s/step - loss: 0.1862 - accuracy: 0.9404 - val_loss: 0.5177 - val_accuracy: 0.8750\n",
      "Epoch 72/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1381 - accuracy: 0.9511\n",
      "Epoch 72: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 105s 5s/step - loss: 0.1381 - accuracy: 0.9511 - val_loss: 0.6669 - val_accuracy: 0.8438\n",
      "Epoch 73/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1485 - accuracy: 0.9648\n",
      "Epoch 73: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 104s 5s/step - loss: 0.1485 - accuracy: 0.9648 - val_loss: 0.6239 - val_accuracy: 0.8562\n",
      "Epoch 74/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1627 - accuracy: 0.9541\n",
      "Epoch 74: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 103s 5s/step - loss: 0.1627 - accuracy: 0.9541 - val_loss: 0.6139 - val_accuracy: 0.8687\n",
      "Epoch 75/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2116 - accuracy: 0.9388\n",
      "Epoch 75: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 104s 5s/step - loss: 0.2116 - accuracy: 0.9388 - val_loss: 0.5563 - val_accuracy: 0.8438\n",
      "Epoch 76/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1808 - accuracy: 0.9419\n",
      "Epoch 76: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 106s 5s/step - loss: 0.1808 - accuracy: 0.9419 - val_loss: 0.5156 - val_accuracy: 0.8750\n",
      "Epoch 77/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2027 - accuracy: 0.9327\n",
      "Epoch 77: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 104s 5s/step - loss: 0.2027 - accuracy: 0.9327 - val_loss: 0.4479 - val_accuracy: 0.8938\n",
      "Epoch 78/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1399 - accuracy: 0.9541\n",
      "Epoch 78: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 103s 5s/step - loss: 0.1399 - accuracy: 0.9541 - val_loss: 0.5107 - val_accuracy: 0.8687\n",
      "Epoch 79/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1615 - accuracy: 0.9388\n",
      "Epoch 79: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 105s 5s/step - loss: 0.1615 - accuracy: 0.9388 - val_loss: 0.5443 - val_accuracy: 0.8625\n",
      "Epoch 80/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2115 - accuracy: 0.9327\n",
      "Epoch 80: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 117s 6s/step - loss: 0.2115 - accuracy: 0.9327 - val_loss: 0.4890 - val_accuracy: 0.8875\n",
      "Epoch 81/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2031 - accuracy: 0.9327\n",
      "Epoch 81: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 115s 6s/step - loss: 0.2031 - accuracy: 0.9327 - val_loss: 0.5812 - val_accuracy: 0.9000\n",
      "Epoch 82/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1602 - accuracy: 0.9465\n",
      "Epoch 82: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 108s 5s/step - loss: 0.1602 - accuracy: 0.9465 - val_loss: 0.4752 - val_accuracy: 0.9000\n",
      "Epoch 83/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1036 - accuracy: 0.9648\n",
      "Epoch 83: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 105s 5s/step - loss: 0.1036 - accuracy: 0.9648 - val_loss: 0.5709 - val_accuracy: 0.8687\n",
      "Epoch 84/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1234 - accuracy: 0.9572\n",
      "Epoch 84: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 110s 5s/step - loss: 0.1234 - accuracy: 0.9572 - val_loss: 0.5729 - val_accuracy: 0.8500\n",
      "Epoch 85/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1256 - accuracy: 0.9572\n",
      "Epoch 85: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 104s 5s/step - loss: 0.1256 - accuracy: 0.9572 - val_loss: 0.6335 - val_accuracy: 0.8687\n",
      "Epoch 86/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1276 - accuracy: 0.9572\n",
      "Epoch 86: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 116s 6s/step - loss: 0.1276 - accuracy: 0.9572 - val_loss: 0.5454 - val_accuracy: 0.8500\n",
      "Epoch 87/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1473 - accuracy: 0.9541\n",
      "Epoch 87: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 103s 5s/step - loss: 0.1473 - accuracy: 0.9541 - val_loss: 0.6051 - val_accuracy: 0.8625\n",
      "Epoch 88/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1579 - accuracy: 0.9480\n",
      "Epoch 88: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 103s 5s/step - loss: 0.1579 - accuracy: 0.9480 - val_loss: 0.6674 - val_accuracy: 0.8062\n",
      "Epoch 89/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1533 - accuracy: 0.9509\n",
      "Epoch 89: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 103s 5s/step - loss: 0.1533 - accuracy: 0.9509 - val_loss: 0.5636 - val_accuracy: 0.8500\n",
      "Epoch 90/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1400 - accuracy: 0.9572\n",
      "Epoch 90: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.1400 - accuracy: 0.9572 - val_loss: 0.6197 - val_accuracy: 0.8687\n",
      "Epoch 91/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1883 - accuracy: 0.9450\n",
      "Epoch 91: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 108s 5s/step - loss: 0.1883 - accuracy: 0.9450 - val_loss: 0.6345 - val_accuracy: 0.8687\n",
      "Epoch 92/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1483 - accuracy: 0.9526\n",
      "Epoch 92: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 99s 5s/step - loss: 0.1483 - accuracy: 0.9526 - val_loss: 0.6707 - val_accuracy: 0.8438\n",
      "Epoch 93/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1438 - accuracy: 0.9511\n",
      "Epoch 93: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 96s 5s/step - loss: 0.1438 - accuracy: 0.9511 - val_loss: 0.5841 - val_accuracy: 0.8625\n",
      "Epoch 94/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1472 - accuracy: 0.9572\n",
      "Epoch 94: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 95s 5s/step - loss: 0.1472 - accuracy: 0.9572 - val_loss: 0.5207 - val_accuracy: 0.8687\n",
      "Epoch 95/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1192 - accuracy: 0.9587\n",
      "Epoch 95: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 100s 5s/step - loss: 0.1192 - accuracy: 0.9587 - val_loss: 0.4561 - val_accuracy: 0.8813\n",
      "Epoch 96/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1092 - accuracy: 0.9633\n",
      "Epoch 96: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 120s 6s/step - loss: 0.1092 - accuracy: 0.9633 - val_loss: 0.6859 - val_accuracy: 0.8313\n",
      "Epoch 97/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1039 - accuracy: 0.9648\n",
      "Epoch 97: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 125s 6s/step - loss: 0.1039 - accuracy: 0.9648 - val_loss: 0.8119 - val_accuracy: 0.8500\n",
      "Epoch 98/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1822 - accuracy: 0.9419\n",
      "Epoch 98: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 115s 6s/step - loss: 0.1822 - accuracy: 0.9419 - val_loss: 0.7179 - val_accuracy: 0.8625\n",
      "Epoch 99/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1259 - accuracy: 0.9526\n",
      "Epoch 99: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 111s 5s/step - loss: 0.1259 - accuracy: 0.9526 - val_loss: 0.8059 - val_accuracy: 0.8188\n",
      "Epoch 100/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1333 - accuracy: 0.9618\n",
      "Epoch 100: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 105s 5s/step - loss: 0.1333 - accuracy: 0.9618 - val_loss: 0.6515 - val_accuracy: 0.8813\n",
      "Epoch 101/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1164 - accuracy: 0.9709\n",
      "Epoch 101: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 106s 5s/step - loss: 0.1164 - accuracy: 0.9709 - val_loss: 0.7208 - val_accuracy: 0.8375\n",
      "Epoch 102/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1676 - accuracy: 0.9434\n",
      "Epoch 102: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 105s 5s/step - loss: 0.1676 - accuracy: 0.9434 - val_loss: 0.5661 - val_accuracy: 0.8750\n",
      "Epoch 103/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1499 - accuracy: 0.9526\n",
      "Epoch 103: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 107s 5s/step - loss: 0.1499 - accuracy: 0.9526 - val_loss: 0.7393 - val_accuracy: 0.8500\n",
      "Epoch 104/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1491 - accuracy: 0.9495\n",
      "Epoch 104: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 105s 5s/step - loss: 0.1491 - accuracy: 0.9495 - val_loss: 0.4893 - val_accuracy: 0.8938\n",
      "Epoch 105/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1190 - accuracy: 0.9541\n",
      "Epoch 105: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 107s 5s/step - loss: 0.1190 - accuracy: 0.9541 - val_loss: 0.4536 - val_accuracy: 0.8813\n",
      "Epoch 106/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1147 - accuracy: 0.9541\n",
      "Epoch 106: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 106s 5s/step - loss: 0.1147 - accuracy: 0.9541 - val_loss: 0.5282 - val_accuracy: 0.8875\n",
      "Epoch 107/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1703 - accuracy: 0.9465\n",
      "Epoch 107: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 109s 5s/step - loss: 0.1703 - accuracy: 0.9465 - val_loss: 0.7294 - val_accuracy: 0.8687\n",
      "Epoch 108/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1318 - accuracy: 0.9572\n",
      "Epoch 108: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 107s 5s/step - loss: 0.1318 - accuracy: 0.9572 - val_loss: 0.6715 - val_accuracy: 0.8438\n",
      "Epoch 109/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1669 - accuracy: 0.9495\n",
      "Epoch 109: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 107s 5s/step - loss: 0.1669 - accuracy: 0.9495 - val_loss: 0.4383 - val_accuracy: 0.8813\n",
      "Epoch 110/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1021 - accuracy: 0.9679\n",
      "Epoch 110: val_loss did not improve from 0.41056\n",
      "21/21 [==============================] - 108s 5s/step - loss: 0.1021 - accuracy: 0.9679 - val_loss: 0.5664 - val_accuracy: 0.8625\n",
      "Epoch 111/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1271 - accuracy: 0.9613\n",
      "Epoch 111: val_loss improved from 0.41056 to 0.40252, saving model to ./model\\111-0.4025.h5\n",
      "21/21 [==============================] - 116s 6s/step - loss: 0.1271 - accuracy: 0.9613 - val_loss: 0.4025 - val_accuracy: 0.8875\n",
      "Epoch 112/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1293 - accuracy: 0.9541\n",
      "Epoch 112: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 106s 5s/step - loss: 0.1293 - accuracy: 0.9541 - val_loss: 0.7565 - val_accuracy: 0.8125\n",
      "Epoch 113/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1181 - accuracy: 0.9602\n",
      "Epoch 113: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 106s 5s/step - loss: 0.1181 - accuracy: 0.9602 - val_loss: 0.5500 - val_accuracy: 0.8625\n",
      "Epoch 114/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1039 - accuracy: 0.9755\n",
      "Epoch 114: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 105s 5s/step - loss: 0.1039 - accuracy: 0.9755 - val_loss: 0.5633 - val_accuracy: 0.8875\n",
      "Epoch 115/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0634 - accuracy: 0.9771\n",
      "Epoch 115: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 107s 5s/step - loss: 0.0634 - accuracy: 0.9771 - val_loss: 0.5571 - val_accuracy: 0.8813\n",
      "Epoch 116/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1403 - accuracy: 0.9572\n",
      "Epoch 116: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 105s 5s/step - loss: 0.1403 - accuracy: 0.9572 - val_loss: 0.6777 - val_accuracy: 0.8562\n",
      "Epoch 117/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1042 - accuracy: 0.9709\n",
      "Epoch 117: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 105s 5s/step - loss: 0.1042 - accuracy: 0.9709 - val_loss: 0.5109 - val_accuracy: 0.8813\n",
      "Epoch 118/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2051 - accuracy: 0.9388\n",
      "Epoch 118: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 106s 5s/step - loss: 0.2051 - accuracy: 0.9388 - val_loss: 0.6203 - val_accuracy: 0.8500\n",
      "Epoch 119/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1486 - accuracy: 0.9526\n",
      "Epoch 119: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 104s 5s/step - loss: 0.1486 - accuracy: 0.9526 - val_loss: 0.5555 - val_accuracy: 0.8687\n",
      "Epoch 120/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1709 - accuracy: 0.9557\n",
      "Epoch 120: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 106s 5s/step - loss: 0.1709 - accuracy: 0.9557 - val_loss: 0.7897 - val_accuracy: 0.8500\n",
      "Epoch 121/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1558 - accuracy: 0.9511\n",
      "Epoch 121: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 107s 5s/step - loss: 0.1558 - accuracy: 0.9511 - val_loss: 0.6825 - val_accuracy: 0.8375\n",
      "Epoch 122/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0908 - accuracy: 0.9786\n",
      "Epoch 122: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 102s 5s/step - loss: 0.0908 - accuracy: 0.9786 - val_loss: 0.6866 - val_accuracy: 0.8500\n",
      "Epoch 123/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1158 - accuracy: 0.9602\n",
      "Epoch 123: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 102s 5s/step - loss: 0.1158 - accuracy: 0.9602 - val_loss: 0.6619 - val_accuracy: 0.8625\n",
      "Epoch 124/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0928 - accuracy: 0.9679\n",
      "Epoch 124: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 103s 5s/step - loss: 0.0928 - accuracy: 0.9679 - val_loss: 0.7487 - val_accuracy: 0.8750\n",
      "Epoch 125/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9709\n",
      "Epoch 125: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 101s 5s/step - loss: 0.0675 - accuracy: 0.9709 - val_loss: 0.5936 - val_accuracy: 0.8750\n",
      "Epoch 126/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0942 - accuracy: 0.9633\n",
      "Epoch 126: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 102s 5s/step - loss: 0.0942 - accuracy: 0.9633 - val_loss: 0.6576 - val_accuracy: 0.8750\n",
      "Epoch 127/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1176 - accuracy: 0.9725\n",
      "Epoch 127: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 102s 5s/step - loss: 0.1176 - accuracy: 0.9725 - val_loss: 0.6093 - val_accuracy: 0.8625\n",
      "Epoch 128/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1104 - accuracy: 0.9633\n",
      "Epoch 128: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 102s 5s/step - loss: 0.1104 - accuracy: 0.9633 - val_loss: 0.9214 - val_accuracy: 0.8188\n",
      "Epoch 129/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1211 - accuracy: 0.9648\n",
      "Epoch 129: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 102s 5s/step - loss: 0.1211 - accuracy: 0.9648 - val_loss: 0.5990 - val_accuracy: 0.8625\n",
      "Epoch 130/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0950 - accuracy: 0.9664\n",
      "Epoch 130: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 102s 5s/step - loss: 0.0950 - accuracy: 0.9664 - val_loss: 0.6812 - val_accuracy: 0.8875\n",
      "Epoch 131/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.9771\n",
      "Epoch 131: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 102s 5s/step - loss: 0.0733 - accuracy: 0.9771 - val_loss: 0.6818 - val_accuracy: 0.8313\n",
      "Epoch 132/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0516 - accuracy: 0.9862\n",
      "Epoch 132: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 101s 5s/step - loss: 0.0516 - accuracy: 0.9862 - val_loss: 0.5591 - val_accuracy: 0.8813\n",
      "Epoch 133/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0614 - accuracy: 0.9836\n",
      "Epoch 133: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 99s 5s/step - loss: 0.0614 - accuracy: 0.9836 - val_loss: 0.6497 - val_accuracy: 0.8687\n",
      "Epoch 134/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0557 - accuracy: 0.9817\n",
      "Epoch 134: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.0557 - accuracy: 0.9817 - val_loss: 0.6332 - val_accuracy: 0.9062\n",
      "Epoch 135/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1424 - accuracy: 0.9511\n",
      "Epoch 135: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.1424 - accuracy: 0.9511 - val_loss: 0.6728 - val_accuracy: 0.8750\n",
      "Epoch 136/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1143 - accuracy: 0.9618\n",
      "Epoch 136: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.1143 - accuracy: 0.9618 - val_loss: 0.5723 - val_accuracy: 0.8938\n",
      "Epoch 137/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1103 - accuracy: 0.9602\n",
      "Epoch 137: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.1103 - accuracy: 0.9602 - val_loss: 0.6487 - val_accuracy: 0.8750\n",
      "Epoch 138/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9709\n",
      "Epoch 138: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.0726 - accuracy: 0.9709 - val_loss: 0.7824 - val_accuracy: 0.8687\n",
      "Epoch 139/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0934 - accuracy: 0.9725\n",
      "Epoch 139: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 93s 4s/step - loss: 0.0934 - accuracy: 0.9725 - val_loss: 0.9653 - val_accuracy: 0.8125\n",
      "Epoch 140/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0802 - accuracy: 0.9725\n",
      "Epoch 140: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.0802 - accuracy: 0.9725 - val_loss: 0.6128 - val_accuracy: 0.8938\n",
      "Epoch 141/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9832\n",
      "Epoch 141: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.0735 - accuracy: 0.9832 - val_loss: 0.6233 - val_accuracy: 0.8562\n",
      "Epoch 142/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0490 - accuracy: 0.9801\n",
      "Epoch 142: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.0490 - accuracy: 0.9801 - val_loss: 0.5671 - val_accuracy: 0.8750\n",
      "Epoch 143/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0827 - accuracy: 0.9755\n",
      "Epoch 143: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.0827 - accuracy: 0.9755 - val_loss: 0.6225 - val_accuracy: 0.8813\n",
      "Epoch 144/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0496 - accuracy: 0.9832\n",
      "Epoch 144: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.0496 - accuracy: 0.9832 - val_loss: 0.5867 - val_accuracy: 0.8625\n",
      "Epoch 145/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.9862\n",
      "Epoch 145: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.0521 - accuracy: 0.9862 - val_loss: 0.6122 - val_accuracy: 0.8562\n",
      "Epoch 146/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0627 - accuracy: 0.9771\n",
      "Epoch 146: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.0627 - accuracy: 0.9771 - val_loss: 0.6191 - val_accuracy: 0.8500\n",
      "Epoch 147/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0652 - accuracy: 0.9786\n",
      "Epoch 147: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.0652 - accuracy: 0.9786 - val_loss: 0.5809 - val_accuracy: 0.8625\n",
      "Epoch 148/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0533 - accuracy: 0.9817\n",
      "Epoch 148: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.0533 - accuracy: 0.9817 - val_loss: 0.6331 - val_accuracy: 0.8625\n",
      "Epoch 149/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0526 - accuracy: 0.9801\n",
      "Epoch 149: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 93s 4s/step - loss: 0.0526 - accuracy: 0.9801 - val_loss: 0.6890 - val_accuracy: 0.8313\n",
      "Epoch 150/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0786 - accuracy: 0.9771\n",
      "Epoch 150: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.0786 - accuracy: 0.9771 - val_loss: 0.7015 - val_accuracy: 0.8313\n",
      "Epoch 151/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0535 - accuracy: 0.9801\n",
      "Epoch 151: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.0535 - accuracy: 0.9801 - val_loss: 0.5386 - val_accuracy: 0.8500\n",
      "Epoch 152/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0581 - accuracy: 0.9801\n",
      "Epoch 152: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.0581 - accuracy: 0.9801 - val_loss: 0.7107 - val_accuracy: 0.8500\n",
      "Epoch 153/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0656 - accuracy: 0.9786\n",
      "Epoch 153: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.0656 - accuracy: 0.9786 - val_loss: 0.6626 - val_accuracy: 0.8750\n",
      "Epoch 154/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0825 - accuracy: 0.9755\n",
      "Epoch 154: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.0825 - accuracy: 0.9755 - val_loss: 0.7441 - val_accuracy: 0.8562\n",
      "Epoch 155/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0540 - accuracy: 0.9807\n",
      "Epoch 155: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 96s 5s/step - loss: 0.0540 - accuracy: 0.9807 - val_loss: 0.7375 - val_accuracy: 0.8750\n",
      "Epoch 156/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0563 - accuracy: 0.9817\n",
      "Epoch 156: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 94s 5s/step - loss: 0.0563 - accuracy: 0.9817 - val_loss: 0.7715 - val_accuracy: 0.8625\n",
      "Epoch 157/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9801\n",
      "Epoch 157: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.0689 - accuracy: 0.9801 - val_loss: 0.8274 - val_accuracy: 0.8813\n",
      "Epoch 158/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9709\n",
      "Epoch 158: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.0693 - accuracy: 0.9709 - val_loss: 0.7902 - val_accuracy: 0.8813\n",
      "Epoch 159/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2221 - accuracy: 0.9450\n",
      "Epoch 159: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.2221 - accuracy: 0.9450 - val_loss: 0.6761 - val_accuracy: 0.8687\n",
      "Epoch 160/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1644 - accuracy: 0.9572\n",
      "Epoch 160: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.1644 - accuracy: 0.9572 - val_loss: 0.7436 - val_accuracy: 0.8375\n",
      "Epoch 161/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1108 - accuracy: 0.9709\n",
      "Epoch 161: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.1108 - accuracy: 0.9709 - val_loss: 0.6867 - val_accuracy: 0.8500\n",
      "Epoch 162/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0971 - accuracy: 0.9709\n",
      "Epoch 162: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.0971 - accuracy: 0.9709 - val_loss: 0.6358 - val_accuracy: 0.8813\n",
      "Epoch 163/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0937 - accuracy: 0.9694\n",
      "Epoch 163: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.0937 - accuracy: 0.9694 - val_loss: 0.4789 - val_accuracy: 0.8813\n",
      "Epoch 164/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0769 - accuracy: 0.9694\n",
      "Epoch 164: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.0769 - accuracy: 0.9694 - val_loss: 0.7047 - val_accuracy: 0.8875\n",
      "Epoch 165/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1107 - accuracy: 0.9602\n",
      "Epoch 165: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.1107 - accuracy: 0.9602 - val_loss: 0.7052 - val_accuracy: 0.8750\n",
      "Epoch 166/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0838 - accuracy: 0.9740\n",
      "Epoch 166: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.0838 - accuracy: 0.9740 - val_loss: 0.7840 - val_accuracy: 0.8375\n",
      "Epoch 167/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9817\n",
      "Epoch 167: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.0701 - accuracy: 0.9817 - val_loss: 0.8174 - val_accuracy: 0.8500\n",
      "Epoch 168/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1293 - accuracy: 0.9648\n",
      "Epoch 168: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.1293 - accuracy: 0.9648 - val_loss: 1.1326 - val_accuracy: 0.8313\n",
      "Epoch 169/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1389 - accuracy: 0.9526\n",
      "Epoch 169: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.1389 - accuracy: 0.9526 - val_loss: 0.7983 - val_accuracy: 0.8500\n",
      "Epoch 170/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.9832\n",
      "Epoch 170: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.0733 - accuracy: 0.9832 - val_loss: 0.6639 - val_accuracy: 0.8562\n",
      "Epoch 171/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9725\n",
      "Epoch 171: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.0763 - accuracy: 0.9725 - val_loss: 0.9373 - val_accuracy: 0.8938\n",
      "Epoch 172/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9740\n",
      "Epoch 172: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.0697 - accuracy: 0.9740 - val_loss: 0.7045 - val_accuracy: 0.8813\n",
      "Epoch 173/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0822 - accuracy: 0.9679\n",
      "Epoch 173: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.0822 - accuracy: 0.9679 - val_loss: 0.6314 - val_accuracy: 0.8938\n",
      "Epoch 174/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9755\n",
      "Epoch 174: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.0695 - accuracy: 0.9755 - val_loss: 0.7531 - val_accuracy: 0.8938\n",
      "Epoch 175/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1343 - accuracy: 0.9725\n",
      "Epoch 175: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.1343 - accuracy: 0.9725 - val_loss: 0.7689 - val_accuracy: 0.8687\n",
      "Epoch 176/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9740\n",
      "Epoch 176: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.0723 - accuracy: 0.9740 - val_loss: 1.2414 - val_accuracy: 0.8625\n",
      "Epoch 177/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0834 - accuracy: 0.9777\n",
      "Epoch 177: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 94s 4s/step - loss: 0.0834 - accuracy: 0.9777 - val_loss: 0.8898 - val_accuracy: 0.8562\n",
      "Epoch 178/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0968 - accuracy: 0.9709\n",
      "Epoch 178: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 90s 4s/step - loss: 0.0968 - accuracy: 0.9709 - val_loss: 0.8188 - val_accuracy: 0.8625\n",
      "Epoch 179/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1364 - accuracy: 0.9679\n",
      "Epoch 179: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.1364 - accuracy: 0.9679 - val_loss: 0.5880 - val_accuracy: 0.9062\n",
      "Epoch 180/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9771\n",
      "Epoch 180: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.0721 - accuracy: 0.9771 - val_loss: 0.6040 - val_accuracy: 0.8625\n",
      "Epoch 181/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0642 - accuracy: 0.9862\n",
      "Epoch 181: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.0642 - accuracy: 0.9862 - val_loss: 0.5663 - val_accuracy: 0.8938\n",
      "Epoch 182/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1204 - accuracy: 0.9557\n",
      "Epoch 182: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.1204 - accuracy: 0.9557 - val_loss: 0.5420 - val_accuracy: 0.8500\n",
      "Epoch 183/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9740\n",
      "Epoch 183: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.0755 - accuracy: 0.9740 - val_loss: 0.6069 - val_accuracy: 0.8750\n",
      "Epoch 184/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0584 - accuracy: 0.9801\n",
      "Epoch 184: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.0584 - accuracy: 0.9801 - val_loss: 0.6010 - val_accuracy: 0.8813\n",
      "Epoch 185/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0448 - accuracy: 0.9832\n",
      "Epoch 185: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.0448 - accuracy: 0.9832 - val_loss: 0.6841 - val_accuracy: 0.8625\n",
      "Epoch 186/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0446 - accuracy: 0.9862\n",
      "Epoch 186: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.0446 - accuracy: 0.9862 - val_loss: 0.5762 - val_accuracy: 0.8750\n",
      "Epoch 187/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0489 - accuracy: 0.9786\n",
      "Epoch 187: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.0489 - accuracy: 0.9786 - val_loss: 0.7548 - val_accuracy: 0.8938\n",
      "Epoch 188/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0430 - accuracy: 0.9847\n",
      "Epoch 188: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.0430 - accuracy: 0.9847 - val_loss: 0.8571 - val_accuracy: 0.8562\n",
      "Epoch 189/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0483 - accuracy: 0.9847\n",
      "Epoch 189: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.0483 - accuracy: 0.9847 - val_loss: 0.8298 - val_accuracy: 0.8500\n",
      "Epoch 190/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0398 - accuracy: 0.9878\n",
      "Epoch 190: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.0398 - accuracy: 0.9878 - val_loss: 0.7736 - val_accuracy: 0.8750\n",
      "Epoch 191/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9832\n",
      "Epoch 191: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 93s 4s/step - loss: 0.0691 - accuracy: 0.9832 - val_loss: 0.7730 - val_accuracy: 0.8687\n",
      "Epoch 192/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9801\n",
      "Epoch 192: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.0740 - accuracy: 0.9801 - val_loss: 0.6955 - val_accuracy: 0.8875\n",
      "Epoch 193/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0969 - accuracy: 0.9755\n",
      "Epoch 193: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.0969 - accuracy: 0.9755 - val_loss: 0.8916 - val_accuracy: 0.8750\n",
      "Epoch 194/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0816 - accuracy: 0.9709\n",
      "Epoch 194: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.0816 - accuracy: 0.9709 - val_loss: 0.7615 - val_accuracy: 0.8813\n",
      "Epoch 195/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0621 - accuracy: 0.9832\n",
      "Epoch 195: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.0621 - accuracy: 0.9832 - val_loss: 0.9307 - val_accuracy: 0.8562\n",
      "Epoch 196/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0913 - accuracy: 0.9725\n",
      "Epoch 196: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.0913 - accuracy: 0.9725 - val_loss: 0.6620 - val_accuracy: 0.8875\n",
      "Epoch 197/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0857 - accuracy: 0.9694\n",
      "Epoch 197: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.0857 - accuracy: 0.9694 - val_loss: 1.0062 - val_accuracy: 0.8625\n",
      "Epoch 198/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1046 - accuracy: 0.9771\n",
      "Epoch 198: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.1046 - accuracy: 0.9771 - val_loss: 0.9197 - val_accuracy: 0.8813\n",
      "Epoch 199/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0861 - accuracy: 0.9777\n",
      "Epoch 199: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 93s 4s/step - loss: 0.0861 - accuracy: 0.9777 - val_loss: 0.7271 - val_accuracy: 0.8938\n",
      "Epoch 200/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0794 - accuracy: 0.9709\n",
      "Epoch 200: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.0794 - accuracy: 0.9709 - val_loss: 0.8375 - val_accuracy: 0.8938\n",
      "Epoch 201/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0520 - accuracy: 0.9847\n",
      "Epoch 201: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.0520 - accuracy: 0.9847 - val_loss: 0.9943 - val_accuracy: 0.8687\n",
      "Epoch 202/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0609 - accuracy: 0.9817\n",
      "Epoch 202: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.0609 - accuracy: 0.9817 - val_loss: 0.7582 - val_accuracy: 0.8687\n",
      "Epoch 203/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0489 - accuracy: 0.9832\n",
      "Epoch 203: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.0489 - accuracy: 0.9832 - val_loss: 1.0242 - val_accuracy: 0.8625\n",
      "Epoch 204/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0504 - accuracy: 0.9801\n",
      "Epoch 204: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.0504 - accuracy: 0.9801 - val_loss: 1.2042 - val_accuracy: 0.8687\n",
      "Epoch 205/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0901 - accuracy: 0.9694\n",
      "Epoch 205: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.0901 - accuracy: 0.9694 - val_loss: 0.8359 - val_accuracy: 0.8750\n",
      "Epoch 206/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9817\n",
      "Epoch 206: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.0671 - accuracy: 0.9817 - val_loss: 0.9345 - val_accuracy: 0.8750\n",
      "Epoch 207/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9817\n",
      "Epoch 207: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.0695 - accuracy: 0.9817 - val_loss: 0.8184 - val_accuracy: 0.8813\n",
      "Epoch 208/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0415 - accuracy: 0.9847\n",
      "Epoch 208: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.0415 - accuracy: 0.9847 - val_loss: 1.0787 - val_accuracy: 0.8375\n",
      "Epoch 209/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9801\n",
      "Epoch 209: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.0685 - accuracy: 0.9801 - val_loss: 0.8397 - val_accuracy: 0.9125\n",
      "Epoch 210/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0891 - accuracy: 0.9771\n",
      "Epoch 210: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.0891 - accuracy: 0.9771 - val_loss: 0.6339 - val_accuracy: 0.8625\n",
      "Epoch 211/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0806 - accuracy: 0.9786\n",
      "Epoch 211: val_loss did not improve from 0.40252\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.0806 - accuracy: 0.9786 - val_loss: 0.5694 - val_accuracy: 0.8875\n"
     ]
    }
   ],
   "source": [
    "train_step = num_train//BATCH_SIZE\n",
    "val_step = num_val//BATCH_SIZE\n",
    "\n",
    "filepath = \"./model/{epoch:03d}-{val_loss:.4f}.h5\"\n",
    "check_point = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=filepath,\n",
    "    save_best_only = True,\n",
    "    verbose = True\n",
    ")\n",
    "early_stop_point = keras.callbacks.EarlyStopping(patience=100)\n",
    "\n",
    "hist = finetune_model.fit(\n",
    "    train_ds, epochs=1000,\n",
    "    steps_per_epoch=train_step,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps=val_step,\n",
    "    callbacks = [check_point, early_stop_point]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38d9dda6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history[\"loss\"], c='b', label=\"train_loss\")\n",
    "plt.plot(hist.history[\"val_loss\"], c='r', label=\"val loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7b6829",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
