{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4642be84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af512893",
   "metadata": {},
   "source": [
    "# tf input pipline\n",
    "## tf.data.dataset 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2019b503",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"./dataset/main_directory/\"\n",
    "test_path = \"./dataset/test/\"\n",
    "\n",
    "INPUT_SHAPE = (224,224,3)\n",
    "BATCH_SIZE = 32\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "tf.random.set_seed(7)\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0779fdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_image(filename):\n",
    "    img = tf.io.read_file(filename)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.resize(img,INPUT_SHAPE[:2])\n",
    "    return img\n",
    "\n",
    "def make_dataset(filepaths,labels):\n",
    "    filenames_ds = tf.data.Dataset.from_tensor_slices(filepaths)\n",
    "    images_ds = filenames_ds.map(\n",
    "        parse_image,\n",
    "        num_parallel_calls=AUTOTUNE\n",
    "    )\n",
    "    labels_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    ds = tf.data.Dataset.zip((images_ds, labels_ds))\n",
    "    return ds\n",
    "\n",
    "def configure_for_performance(ds):\n",
    "    ds = ds.shuffle(buffer_size=1000)\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    ds = ds.repeat()\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29e61416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of classes : 11\n",
      "number of images : 858\n",
      "./dataset/main_directory\\1\\1_train_031.png\n",
      "1\n",
      "\n",
      "data 5 preview\n",
      "./dataset/main_directory\\5\\5_train_617.png\n",
      "class : 5\n",
      "./dataset/main_directory\\8\\8_train_838.png\n",
      "class : 8\n",
      "./dataset/main_directory\\9\\9_train_387.png\n",
      "class : 9\n",
      "./dataset/main_directory\\8\\8_train_007.png\n",
      "class : 8\n",
      "./dataset/main_directory\\4\\4_train_504.png\n",
      "class : 4\n"
     ]
    }
   ],
   "source": [
    "classes = os.listdir(train_path)\n",
    "num_classes = len(classes)\n",
    "print(\"number of classes :\", num_classes)\n",
    "\n",
    "filenames = glob(train_path+'*/*')\n",
    "num_images = len(filenames)\n",
    "print(\"number of images :\", num_images)\n",
    "\n",
    "print(filenames[0])\n",
    "print(filenames[0].split(os.sep)[-2])\n",
    "print()\n",
    "\n",
    "np.random.shuffle(filenames)\n",
    "labels = [classes.index(fn.split(os.sep)[-2]) for fn in filenames]\n",
    "\n",
    "print(\"data 5 preview\")\n",
    "for path,label in zip(filenames[:5],labels[:5]):\n",
    "    print(path)\n",
    "    print(\"class :\",classes[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52580d6a",
   "metadata": {},
   "source": [
    "# 학습셋, 검증셋으로 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89aaea30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training data : 686\n",
      "number of validation data : 172\n",
      "data 5 preview\n",
      "./dataset/main_directory\\8\\8_train_703.png\n",
      "class : 8\n",
      "./dataset/main_directory\\5\\5_train_442.png\n",
      "class : 5\n",
      "./dataset/main_directory\\5\\5_train_611.png\n",
      "class : 5\n",
      "./dataset/main_directory\\4\\4_train_796.png\n",
      "class : 4\n",
      "./dataset/main_directory\\10-1\\10-1_train_390.png\n",
      "class : 10-1\n"
     ]
    }
   ],
   "source": [
    "train_x, val_x, train_y, val_y = train_test_split(\n",
    "    filenames, labels, test_size=0.2,\n",
    "    stratify=labels, random_state=1\n",
    ")\n",
    "num_train = len(train_x)\n",
    "num_val = len(val_x)\n",
    "print(\"number of training data :\", num_train)\n",
    "print(\"number of validation data :\", num_val)\n",
    "\n",
    "print(\"data 5 preview\")\n",
    "for path,label in zip(train_x[:5],train_y[:5]):\n",
    "    print(path)\n",
    "    print(\"class :\",classes[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad436c5",
   "metadata": {},
   "source": [
    "# tf dataset 객체 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28a1f9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = make_dataset(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ecf94b",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# tf.data.dataset 확인\n",
    "\n",
    "for img, label in train_ds.take(5):\n",
    "    # print(img.numpy().shape)\n",
    "    img = img.numpy()\n",
    "    # print(img.min(), img.max())\n",
    "    \n",
    "    img = img.astype(np.uint8)\n",
    "    idx = (label.numpy())\n",
    "    \n",
    "    plt.imshow(img), plt.axis('off')\n",
    "    plt.title(classes[idx])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e764d72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = make_dataset(val_x, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a3e5691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 성능 높이기\n",
    "train_ds = configure_for_performance(train_ds)\n",
    "val_ds = configure_for_performance(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1974c928",
   "metadata": {},
   "source": [
    "tf.data.experimental.save(train_ds, \"train_ds\", compression=\"GZIP\")\n",
    "tf.data.experimental.save(val_ds, \"val_ds\", compression=\"GZIP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6d4c0b",
   "metadata": {},
   "source": [
    "train_ds = tf.data.experimental.load(\"train_ds\", compression=\"GZIP\")\n",
    "val_ds = tf.data.experimental.load(\"val_ds\", compression=\"GZIP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b12369",
   "metadata": {},
   "source": [
    "# data augmentation layer\n",
    "## evaluate() 또는 predict() 호출 시에는 자동으로 비활성화 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ee8c6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential([\n",
    "    keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "    keras.layers.experimental.preprocessing.RandomTranslation(0.1,0.1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc848d0",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "103a8d62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transfer_model = keras.applications.Xception(\n",
    "    input_shape= INPUT_SHAPE,\n",
    "    include_top= False,\n",
    "    weights= 'imagenet',\n",
    ")\n",
    "transfer_model.trainable = False\n",
    "# transfer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ac2f51f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "finetune_model = keras.Sequential([\n",
    "    keras.Input(shape=INPUT_SHAPE),\n",
    "    data_augmentation,\n",
    "    keras.layers.experimental.preprocessing.Rescaling(1./255),\n",
    "    transfer_model,\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(2000, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(1000, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "# finetune_model.summary()\n",
    "\n",
    "finetune_model.compile(loss='sparse_categorical_crossentropy',\n",
    "                      optimizer=keras.optimizers.Adam(learning_rate=0.0002),\n",
    "                       metrics='accuracy')                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d5fbc2",
   "metadata": {},
   "source": [
    "# batch 데이터셋 확인\n",
    "\n",
    "for batch in train_ds.take(1):\n",
    "    for i in range(5):\n",
    "        img = batch[0][i]\n",
    "        # print(img.numpy().shape)\n",
    "        img = img.numpy()\n",
    "        # print(img.min(), img.max())\n",
    "        \n",
    "        img = img.astype(np.uint8)\n",
    "        \n",
    "        label = batch[1][i]\n",
    "        idx = (label.numpy())\n",
    "        \n",
    "        plt.imshow(img), plt.axis('off')\n",
    "        plt.title(classes[idx])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3499551b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 2.9303 - accuracy: 0.1533\n",
      "Epoch 1: val_loss improved from inf to 2.26294, saving model to ./model\\001-2.2629.h5\n",
      "21/21 [==============================] - 160s 7s/step - loss: 2.9303 - accuracy: 0.1533 - val_loss: 2.2629 - val_accuracy: 0.1750\n",
      "Epoch 2/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 2.2977 - accuracy: 0.1911\n",
      "Epoch 2: val_loss improved from 2.26294 to 2.22319, saving model to ./model\\002-2.2232.h5\n",
      "21/21 [==============================] - 162s 8s/step - loss: 2.2977 - accuracy: 0.1911 - val_loss: 2.2232 - val_accuracy: 0.2188\n",
      "Epoch 3/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 2.2543 - accuracy: 0.2278\n",
      "Epoch 3: val_loss improved from 2.22319 to 1.91595, saving model to ./model\\003-1.9160.h5\n",
      "21/21 [==============================] - 152s 7s/step - loss: 2.2543 - accuracy: 0.2278 - val_loss: 1.9160 - val_accuracy: 0.2750\n",
      "Epoch 4/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 2.0174 - accuracy: 0.2829\n",
      "Epoch 4: val_loss improved from 1.91595 to 1.69024, saving model to ./model\\004-1.6902.h5\n",
      "21/21 [==============================] - 187s 9s/step - loss: 2.0174 - accuracy: 0.2829 - val_loss: 1.6902 - val_accuracy: 0.4062\n",
      "Epoch 5/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.8403 - accuracy: 0.3394\n",
      "Epoch 5: val_loss did not improve from 1.69024\n",
      "21/21 [==============================] - 101s 5s/step - loss: 1.8403 - accuracy: 0.3394 - val_loss: 1.7358 - val_accuracy: 0.4000\n",
      "Epoch 6/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.6857 - accuracy: 0.4159\n",
      "Epoch 6: val_loss improved from 1.69024 to 1.43781, saving model to ./model\\006-1.4378.h5\n",
      "21/21 [==============================] - 108s 5s/step - loss: 1.6857 - accuracy: 0.4159 - val_loss: 1.4378 - val_accuracy: 0.4812\n",
      "Epoch 7/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5391 - accuracy: 0.4526\n",
      "Epoch 7: val_loss improved from 1.43781 to 1.23429, saving model to ./model\\007-1.2343.h5\n",
      "21/21 [==============================] - 135s 6s/step - loss: 1.5391 - accuracy: 0.4526 - val_loss: 1.2343 - val_accuracy: 0.5813\n",
      "Epoch 8/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.4289 - accuracy: 0.5031\n",
      "Epoch 8: val_loss improved from 1.23429 to 1.17175, saving model to ./model\\008-1.1718.h5\n",
      "21/21 [==============================] - 135s 7s/step - loss: 1.4289 - accuracy: 0.5031 - val_loss: 1.1718 - val_accuracy: 0.5813\n",
      "Epoch 9/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.3299 - accuracy: 0.5428\n",
      "Epoch 9: val_loss improved from 1.17175 to 0.99123, saving model to ./model\\009-0.9912.h5\n",
      "21/21 [==============================] - 153s 7s/step - loss: 1.3299 - accuracy: 0.5428 - val_loss: 0.9912 - val_accuracy: 0.6187\n",
      "Epoch 10/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2871 - accuracy: 0.5321\n",
      "Epoch 10: val_loss improved from 0.99123 to 0.96125, saving model to ./model\\010-0.9613.h5\n",
      "21/21 [==============================] - 196s 9s/step - loss: 1.2871 - accuracy: 0.5321 - val_loss: 0.9613 - val_accuracy: 0.6062\n",
      "Epoch 11/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1358 - accuracy: 0.5887\n",
      "Epoch 11: val_loss did not improve from 0.96125\n",
      "21/21 [==============================] - 112s 5s/step - loss: 1.1358 - accuracy: 0.5887 - val_loss: 0.9638 - val_accuracy: 0.6313\n",
      "Epoch 12/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1162 - accuracy: 0.6116\n",
      "Epoch 12: val_loss improved from 0.96125 to 0.92919, saving model to ./model\\012-0.9292.h5\n",
      "21/21 [==============================] - 133s 6s/step - loss: 1.1162 - accuracy: 0.6116 - val_loss: 0.9292 - val_accuracy: 0.6812\n",
      "Epoch 13/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0418 - accuracy: 0.6269\n",
      "Epoch 13: val_loss improved from 0.92919 to 0.78776, saving model to ./model\\013-0.7878.h5\n",
      "21/21 [==============================] - 154s 7s/step - loss: 1.0418 - accuracy: 0.6269 - val_loss: 0.7878 - val_accuracy: 0.7000\n",
      "Epoch 14/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0592 - accuracy: 0.6346\n",
      "Epoch 14: val_loss did not improve from 0.78776\n",
      "21/21 [==============================] - 120s 5s/step - loss: 1.0592 - accuracy: 0.6346 - val_loss: 0.8401 - val_accuracy: 0.6875\n",
      "Epoch 15/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8998 - accuracy: 0.6865\n",
      "Epoch 15: val_loss improved from 0.78776 to 0.74475, saving model to ./model\\015-0.7447.h5\n",
      "21/21 [==============================] - 115s 6s/step - loss: 0.8998 - accuracy: 0.6865 - val_loss: 0.7447 - val_accuracy: 0.7188\n",
      "Epoch 16/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8839 - accuracy: 0.6713\n",
      "Epoch 16: val_loss did not improve from 0.74475\n",
      "21/21 [==============================] - 120s 6s/step - loss: 0.8839 - accuracy: 0.6713 - val_loss: 0.8005 - val_accuracy: 0.6687\n",
      "Epoch 17/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7346 - accuracy: 0.7324\n",
      "Epoch 17: val_loss improved from 0.74475 to 0.70850, saving model to ./model\\017-0.7085.h5\n",
      "21/21 [==============================] - 158s 8s/step - loss: 0.7346 - accuracy: 0.7324 - val_loss: 0.7085 - val_accuracy: 0.7250\n",
      "Epoch 18/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7807 - accuracy: 0.7110\n",
      "Epoch 18: val_loss improved from 0.70850 to 0.64310, saving model to ./model\\018-0.6431.h5\n",
      "21/21 [==============================] - 175s 8s/step - loss: 0.7807 - accuracy: 0.7110 - val_loss: 0.6431 - val_accuracy: 0.7688\n",
      "Epoch 19/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6883 - accuracy: 0.7431\n",
      "Epoch 19: val_loss did not improve from 0.64310\n",
      "21/21 [==============================] - 108s 5s/step - loss: 0.6883 - accuracy: 0.7431 - val_loss: 0.6591 - val_accuracy: 0.7563\n",
      "Epoch 20/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6859 - accuracy: 0.7477\n",
      "Epoch 20: val_loss improved from 0.64310 to 0.62468, saving model to ./model\\020-0.6247.h5\n",
      "21/21 [==============================] - 190s 9s/step - loss: 0.6859 - accuracy: 0.7477 - val_loss: 0.6247 - val_accuracy: 0.7750\n",
      "Epoch 21/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7150 - accuracy: 0.7446\n",
      "Epoch 21: val_loss improved from 0.62468 to 0.60504, saving model to ./model\\021-0.6050.h5\n",
      "21/21 [==============================] - 233s 11s/step - loss: 0.7150 - accuracy: 0.7446 - val_loss: 0.6050 - val_accuracy: 0.7625\n",
      "Epoch 22/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6756 - accuracy: 0.7370\n",
      "Epoch 22: val_loss did not improve from 0.60504\n",
      "21/21 [==============================] - 111s 5s/step - loss: 0.6756 - accuracy: 0.7370 - val_loss: 0.6319 - val_accuracy: 0.7875\n",
      "Epoch 23/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5673 - accuracy: 0.7976\n",
      "Epoch 23: val_loss improved from 0.60504 to 0.52944, saving model to ./model\\023-0.5294.h5\n",
      "21/21 [==============================] - 151s 7s/step - loss: 0.5673 - accuracy: 0.7976 - val_loss: 0.5294 - val_accuracy: 0.7812\n",
      "Epoch 24/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6124 - accuracy: 0.7630\n",
      "Epoch 24: val_loss did not improve from 0.52944\n",
      "21/21 [==============================] - 121s 5s/step - loss: 0.6124 - accuracy: 0.7630 - val_loss: 0.5670 - val_accuracy: 0.8375\n",
      "Epoch 25/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5681 - accuracy: 0.7920\n",
      "Epoch 25: val_loss did not improve from 0.52944\n",
      "21/21 [==============================] - 117s 6s/step - loss: 0.5681 - accuracy: 0.7920 - val_loss: 0.6128 - val_accuracy: 0.7937\n",
      "Epoch 26/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5258 - accuracy: 0.8135\n",
      "Epoch 26: val_loss improved from 0.52944 to 0.52552, saving model to ./model\\026-0.5255.h5\n",
      "21/21 [==============================] - 168s 8s/step - loss: 0.5258 - accuracy: 0.8135 - val_loss: 0.5255 - val_accuracy: 0.8125\n",
      "Epoch 27/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4874 - accuracy: 0.8196\n",
      "Epoch 27: val_loss improved from 0.52552 to 0.45434, saving model to ./model\\027-0.4543.h5\n",
      "21/21 [==============================] - 199s 10s/step - loss: 0.4874 - accuracy: 0.8196 - val_loss: 0.4543 - val_accuracy: 0.8313\n",
      "Epoch 28/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4621 - accuracy: 0.8379\n",
      "Epoch 28: val_loss did not improve from 0.45434\n",
      "21/21 [==============================] - 110s 5s/step - loss: 0.4621 - accuracy: 0.8379 - val_loss: 0.4950 - val_accuracy: 0.8313\n",
      "Epoch 29/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4945 - accuracy: 0.8119\n",
      "Epoch 29: val_loss did not improve from 0.45434\n",
      "21/21 [==============================] - 107s 5s/step - loss: 0.4945 - accuracy: 0.8119 - val_loss: 0.5619 - val_accuracy: 0.7625\n",
      "Epoch 30/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4534 - accuracy: 0.8456\n",
      "Epoch 30: val_loss did not improve from 0.45434\n",
      "21/21 [==============================] - 108s 5s/step - loss: 0.4534 - accuracy: 0.8456 - val_loss: 0.5856 - val_accuracy: 0.7937\n",
      "Epoch 31/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4199 - accuracy: 0.8502\n",
      "Epoch 31: val_loss did not improve from 0.45434\n",
      "21/21 [==============================] - 104s 5s/step - loss: 0.4199 - accuracy: 0.8502 - val_loss: 0.5771 - val_accuracy: 0.8000\n",
      "Epoch 32/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5206 - accuracy: 0.8165\n",
      "Epoch 32: val_loss did not improve from 0.45434\n",
      "21/21 [==============================] - 102s 5s/step - loss: 0.5206 - accuracy: 0.8165 - val_loss: 0.5951 - val_accuracy: 0.7437\n",
      "Epoch 33/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4685 - accuracy: 0.8440\n",
      "Epoch 33: val_loss did not improve from 0.45434\n",
      "21/21 [==============================] - 179s 9s/step - loss: 0.4685 - accuracy: 0.8440 - val_loss: 0.6509 - val_accuracy: 0.7937\n",
      "Epoch 34/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5218 - accuracy: 0.8211\n",
      "Epoch 34: val_loss did not improve from 0.45434\n",
      "21/21 [==============================] - 156s 7s/step - loss: 0.5218 - accuracy: 0.8211 - val_loss: 0.5828 - val_accuracy: 0.8125\n",
      "Epoch 35/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4365 - accuracy: 0.8394\n",
      "Epoch 35: val_loss did not improve from 0.45434\n",
      "21/21 [==============================] - 122s 6s/step - loss: 0.4365 - accuracy: 0.8394 - val_loss: 0.6244 - val_accuracy: 0.7937\n",
      "Epoch 36/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3296 - accuracy: 0.8777\n",
      "Epoch 36: val_loss did not improve from 0.45434\n",
      "21/21 [==============================] - 106s 5s/step - loss: 0.3296 - accuracy: 0.8777 - val_loss: 0.5873 - val_accuracy: 0.8188\n",
      "Epoch 37/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3662 - accuracy: 0.8700\n",
      "Epoch 37: val_loss did not improve from 0.45434\n",
      "21/21 [==============================] - 112s 5s/step - loss: 0.3662 - accuracy: 0.8700 - val_loss: 0.5440 - val_accuracy: 0.8313\n",
      "Epoch 38/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3625 - accuracy: 0.8654\n",
      "Epoch 38: val_loss did not improve from 0.45434\n",
      "21/21 [==============================] - 109s 5s/step - loss: 0.3625 - accuracy: 0.8654 - val_loss: 0.4819 - val_accuracy: 0.8625\n",
      "Epoch 39/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3187 - accuracy: 0.8853\n",
      "Epoch 39: val_loss did not improve from 0.45434\n",
      "21/21 [==============================] - 109s 5s/step - loss: 0.3187 - accuracy: 0.8853 - val_loss: 0.4694 - val_accuracy: 0.8813\n",
      "Epoch 40/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3802 - accuracy: 0.8654\n",
      "Epoch 40: val_loss did not improve from 0.45434\n",
      "21/21 [==============================] - 108s 5s/step - loss: 0.3802 - accuracy: 0.8654 - val_loss: 0.4781 - val_accuracy: 0.8375\n",
      "Epoch 41/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3112 - accuracy: 0.8807\n",
      "Epoch 41: val_loss improved from 0.45434 to 0.44656, saving model to ./model\\041-0.4466.h5\n",
      "21/21 [==============================] - 161s 8s/step - loss: 0.3112 - accuracy: 0.8807 - val_loss: 0.4466 - val_accuracy: 0.8625\n",
      "Epoch 42/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3636 - accuracy: 0.8746\n",
      "Epoch 42: val_loss improved from 0.44656 to 0.43228, saving model to ./model\\042-0.4323.h5\n",
      "21/21 [==============================] - 183s 9s/step - loss: 0.3636 - accuracy: 0.8746 - val_loss: 0.4323 - val_accuracy: 0.8500\n",
      "Epoch 43/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3054 - accuracy: 0.9021\n",
      "Epoch 43: val_loss did not improve from 0.43228\n",
      "21/21 [==============================] - 113s 5s/step - loss: 0.3054 - accuracy: 0.9021 - val_loss: 0.5915 - val_accuracy: 0.8313\n",
      "Epoch 44/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2961 - accuracy: 0.8838\n",
      "Epoch 44: val_loss did not improve from 0.43228\n",
      "21/21 [==============================] - 104s 5s/step - loss: 0.2961 - accuracy: 0.8838 - val_loss: 0.6529 - val_accuracy: 0.8438\n",
      "Epoch 45/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2496 - accuracy: 0.9092\n",
      "Epoch 45: val_loss improved from 0.43228 to 0.34322, saving model to ./model\\045-0.3432.h5\n",
      "21/21 [==============================] - 214s 10s/step - loss: 0.2496 - accuracy: 0.9092 - val_loss: 0.3432 - val_accuracy: 0.8625\n",
      "Epoch 46/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2731 - accuracy: 0.9067\n",
      "Epoch 46: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 103s 5s/step - loss: 0.2731 - accuracy: 0.9067 - val_loss: 0.5076 - val_accuracy: 0.8313\n",
      "Epoch 47/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2890 - accuracy: 0.8838\n",
      "Epoch 47: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 103s 5s/step - loss: 0.2890 - accuracy: 0.8838 - val_loss: 0.5698 - val_accuracy: 0.8438\n",
      "Epoch 48/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2486 - accuracy: 0.9128\n",
      "Epoch 48: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 133s 6s/step - loss: 0.2486 - accuracy: 0.9128 - val_loss: 0.4495 - val_accuracy: 0.8375\n",
      "Epoch 49/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2474 - accuracy: 0.9037\n",
      "Epoch 49: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 122s 6s/step - loss: 0.2474 - accuracy: 0.9037 - val_loss: 0.4508 - val_accuracy: 0.8500\n",
      "Epoch 50/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2399 - accuracy: 0.9052\n",
      "Epoch 50: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 126s 6s/step - loss: 0.2399 - accuracy: 0.9052 - val_loss: 0.5665 - val_accuracy: 0.8562\n",
      "Epoch 51/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3079 - accuracy: 0.8976\n",
      "Epoch 51: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 124s 6s/step - loss: 0.3079 - accuracy: 0.8976 - val_loss: 0.4077 - val_accuracy: 0.8500\n",
      "Epoch 52/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2006 - accuracy: 0.9312\n",
      "Epoch 52: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 114s 5s/step - loss: 0.2006 - accuracy: 0.9312 - val_loss: 0.4410 - val_accuracy: 0.8938\n",
      "Epoch 53/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2277 - accuracy: 0.9174\n",
      "Epoch 53: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 112s 5s/step - loss: 0.2277 - accuracy: 0.9174 - val_loss: 0.5632 - val_accuracy: 0.8562\n",
      "Epoch 54/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2324 - accuracy: 0.9205\n",
      "Epoch 54: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 112s 5s/step - loss: 0.2324 - accuracy: 0.9205 - val_loss: 0.5199 - val_accuracy: 0.8500\n",
      "Epoch 55/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1765 - accuracy: 0.9358\n",
      "Epoch 55: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 115s 5s/step - loss: 0.1765 - accuracy: 0.9358 - val_loss: 0.5560 - val_accuracy: 0.8750\n",
      "Epoch 56/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2139 - accuracy: 0.9220\n",
      "Epoch 56: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 121s 6s/step - loss: 0.2139 - accuracy: 0.9220 - val_loss: 0.4128 - val_accuracy: 0.8687\n",
      "Epoch 57/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1992 - accuracy: 0.9159\n",
      "Epoch 57: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 116s 6s/step - loss: 0.1992 - accuracy: 0.9159 - val_loss: 0.6232 - val_accuracy: 0.8438\n",
      "Epoch 58/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2088 - accuracy: 0.9220\n",
      "Epoch 58: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 108s 5s/step - loss: 0.2088 - accuracy: 0.9220 - val_loss: 0.4471 - val_accuracy: 0.9000\n",
      "Epoch 59/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2351 - accuracy: 0.9190\n",
      "Epoch 59: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 102s 5s/step - loss: 0.2351 - accuracy: 0.9190 - val_loss: 0.4100 - val_accuracy: 0.8750\n",
      "Epoch 60/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2073 - accuracy: 0.9190\n",
      "Epoch 60: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 113s 5s/step - loss: 0.2073 - accuracy: 0.9190 - val_loss: 0.5638 - val_accuracy: 0.8500\n",
      "Epoch 61/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2152 - accuracy: 0.9343\n",
      "Epoch 61: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 115s 6s/step - loss: 0.2152 - accuracy: 0.9343 - val_loss: 0.5141 - val_accuracy: 0.8500\n",
      "Epoch 62/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2022 - accuracy: 0.9266\n",
      "Epoch 62: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 112s 5s/step - loss: 0.2022 - accuracy: 0.9266 - val_loss: 0.5581 - val_accuracy: 0.8562\n",
      "Epoch 63/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1883 - accuracy: 0.9434\n",
      "Epoch 63: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 106s 5s/step - loss: 0.1883 - accuracy: 0.9434 - val_loss: 0.4610 - val_accuracy: 0.8750\n",
      "Epoch 64/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2046 - accuracy: 0.9205\n",
      "Epoch 64: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 113s 5s/step - loss: 0.2046 - accuracy: 0.9205 - val_loss: 0.5280 - val_accuracy: 0.8813\n",
      "Epoch 65/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2340 - accuracy: 0.9312\n",
      "Epoch 65: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 115s 6s/step - loss: 0.2340 - accuracy: 0.9312 - val_loss: 0.5041 - val_accuracy: 0.8687\n",
      "Epoch 66/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1721 - accuracy: 0.9358\n",
      "Epoch 66: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 106s 5s/step - loss: 0.1721 - accuracy: 0.9358 - val_loss: 0.5257 - val_accuracy: 0.8687\n",
      "Epoch 67/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1548 - accuracy: 0.9390\n",
      "Epoch 67: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 106s 5s/step - loss: 0.1548 - accuracy: 0.9390 - val_loss: 0.4450 - val_accuracy: 0.9000\n",
      "Epoch 68/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2097 - accuracy: 0.9281\n",
      "Epoch 68: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 99s 5s/step - loss: 0.2097 - accuracy: 0.9281 - val_loss: 0.5060 - val_accuracy: 0.8938\n",
      "Epoch 69/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1714 - accuracy: 0.9602\n",
      "Epoch 69: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 102s 5s/step - loss: 0.1714 - accuracy: 0.9602 - val_loss: 0.5655 - val_accuracy: 0.8562\n",
      "Epoch 70/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2108 - accuracy: 0.9266\n",
      "Epoch 70: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 97s 5s/step - loss: 0.2108 - accuracy: 0.9266 - val_loss: 0.6040 - val_accuracy: 0.8562\n",
      "Epoch 71/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2010 - accuracy: 0.9434\n",
      "Epoch 71: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.2010 - accuracy: 0.9434 - val_loss: 0.5918 - val_accuracy: 0.8625\n",
      "Epoch 72/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2452 - accuracy: 0.9266\n",
      "Epoch 72: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 102s 5s/step - loss: 0.2452 - accuracy: 0.9266 - val_loss: 0.5746 - val_accuracy: 0.8687\n",
      "Epoch 73/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1705 - accuracy: 0.9388\n",
      "Epoch 73: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 99s 5s/step - loss: 0.1705 - accuracy: 0.9388 - val_loss: 0.7031 - val_accuracy: 0.8438\n",
      "Epoch 74/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1838 - accuracy: 0.9373\n",
      "Epoch 74: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 102s 5s/step - loss: 0.1838 - accuracy: 0.9373 - val_loss: 0.6760 - val_accuracy: 0.8625\n",
      "Epoch 75/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1883 - accuracy: 0.9419\n",
      "Epoch 75: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 104s 5s/step - loss: 0.1883 - accuracy: 0.9419 - val_loss: 0.4473 - val_accuracy: 0.8813\n",
      "Epoch 76/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1673 - accuracy: 0.9404\n",
      "Epoch 76: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 106s 5s/step - loss: 0.1673 - accuracy: 0.9404 - val_loss: 0.5965 - val_accuracy: 0.8813\n",
      "Epoch 77/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1595 - accuracy: 0.9419\n",
      "Epoch 77: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 132s 6s/step - loss: 0.1595 - accuracy: 0.9419 - val_loss: 0.5286 - val_accuracy: 0.8938\n",
      "Epoch 78/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1825 - accuracy: 0.9281\n",
      "Epoch 78: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 101s 5s/step - loss: 0.1825 - accuracy: 0.9281 - val_loss: 0.5440 - val_accuracy: 0.8813\n",
      "Epoch 79/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1339 - accuracy: 0.9480\n",
      "Epoch 79: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 111s 5s/step - loss: 0.1339 - accuracy: 0.9480 - val_loss: 0.6311 - val_accuracy: 0.8750\n",
      "Epoch 80/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1424 - accuracy: 0.9465\n",
      "Epoch 80: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 204s 10s/step - loss: 0.1424 - accuracy: 0.9465 - val_loss: 0.6068 - val_accuracy: 0.8750\n",
      "Epoch 81/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1320 - accuracy: 0.9557\n",
      "Epoch 81: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 171s 8s/step - loss: 0.1320 - accuracy: 0.9557 - val_loss: 0.7563 - val_accuracy: 0.8625\n",
      "Epoch 82/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1408 - accuracy: 0.9526\n",
      "Epoch 82: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 164s 8s/step - loss: 0.1408 - accuracy: 0.9526 - val_loss: 0.7166 - val_accuracy: 0.8562\n",
      "Epoch 83/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1434 - accuracy: 0.9450\n",
      "Epoch 83: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 182s 9s/step - loss: 0.1434 - accuracy: 0.9450 - val_loss: 0.6657 - val_accuracy: 0.8500\n",
      "Epoch 84/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1362 - accuracy: 0.9572\n",
      "Epoch 84: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 186s 9s/step - loss: 0.1362 - accuracy: 0.9572 - val_loss: 0.5371 - val_accuracy: 0.8750\n",
      "Epoch 85/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1920 - accuracy: 0.9434\n",
      "Epoch 85: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 137s 7s/step - loss: 0.1920 - accuracy: 0.9434 - val_loss: 0.7401 - val_accuracy: 0.8500\n",
      "Epoch 86/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1748 - accuracy: 0.9465\n",
      "Epoch 86: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 104s 5s/step - loss: 0.1748 - accuracy: 0.9465 - val_loss: 0.5684 - val_accuracy: 0.8500\n",
      "Epoch 87/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1283 - accuracy: 0.9526\n",
      "Epoch 87: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 107s 5s/step - loss: 0.1283 - accuracy: 0.9526 - val_loss: 0.5022 - val_accuracy: 0.8813\n",
      "Epoch 88/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1004 - accuracy: 0.9618\n",
      "Epoch 88: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 104s 5s/step - loss: 0.1004 - accuracy: 0.9618 - val_loss: 0.5114 - val_accuracy: 0.8750\n",
      "Epoch 89/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1129 - accuracy: 0.9658\n",
      "Epoch 89: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 109s 5s/step - loss: 0.1129 - accuracy: 0.9658 - val_loss: 0.4279 - val_accuracy: 0.9125\n",
      "Epoch 90/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1586 - accuracy: 0.9419\n",
      "Epoch 90: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 101s 5s/step - loss: 0.1586 - accuracy: 0.9419 - val_loss: 0.4873 - val_accuracy: 0.9062\n",
      "Epoch 91/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1632 - accuracy: 0.9297\n",
      "Epoch 91: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 102s 5s/step - loss: 0.1632 - accuracy: 0.9297 - val_loss: 0.5674 - val_accuracy: 0.8750\n",
      "Epoch 92/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1178 - accuracy: 0.9633\n",
      "Epoch 92: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 102s 5s/step - loss: 0.1178 - accuracy: 0.9633 - val_loss: 0.5956 - val_accuracy: 0.8687\n",
      "Epoch 93/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1454 - accuracy: 0.9557\n",
      "Epoch 93: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 103s 5s/step - loss: 0.1454 - accuracy: 0.9557 - val_loss: 0.5163 - val_accuracy: 0.8813\n",
      "Epoch 94/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1481 - accuracy: 0.9557\n",
      "Epoch 94: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 103s 5s/step - loss: 0.1481 - accuracy: 0.9557 - val_loss: 0.5874 - val_accuracy: 0.8625\n",
      "Epoch 95/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1621 - accuracy: 0.9465\n",
      "Epoch 95: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 103s 5s/step - loss: 0.1621 - accuracy: 0.9465 - val_loss: 0.5544 - val_accuracy: 0.8687\n",
      "Epoch 96/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1177 - accuracy: 0.9587\n",
      "Epoch 96: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 102s 5s/step - loss: 0.1177 - accuracy: 0.9587 - val_loss: 0.5679 - val_accuracy: 0.8813\n",
      "Epoch 97/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1398 - accuracy: 0.9572\n",
      "Epoch 97: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 102s 5s/step - loss: 0.1398 - accuracy: 0.9572 - val_loss: 0.6473 - val_accuracy: 0.8750\n",
      "Epoch 98/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1823 - accuracy: 0.9419\n",
      "Epoch 98: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 102s 5s/step - loss: 0.1823 - accuracy: 0.9419 - val_loss: 0.7005 - val_accuracy: 0.8562\n",
      "Epoch 99/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1555 - accuracy: 0.9404\n",
      "Epoch 99: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 103s 5s/step - loss: 0.1555 - accuracy: 0.9404 - val_loss: 0.6518 - val_accuracy: 0.8625\n",
      "Epoch 100/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1498 - accuracy: 0.9480\n",
      "Epoch 100: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 102s 5s/step - loss: 0.1498 - accuracy: 0.9480 - val_loss: 0.4697 - val_accuracy: 0.8875\n",
      "Epoch 101/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1807 - accuracy: 0.9587\n",
      "Epoch 101: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 101s 5s/step - loss: 0.1807 - accuracy: 0.9587 - val_loss: 0.5291 - val_accuracy: 0.8500\n",
      "Epoch 102/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1667 - accuracy: 0.9388\n",
      "Epoch 102: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 106s 5s/step - loss: 0.1667 - accuracy: 0.9388 - val_loss: 0.5816 - val_accuracy: 0.8625\n",
      "Epoch 103/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1195 - accuracy: 0.9526\n",
      "Epoch 103: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 121s 6s/step - loss: 0.1195 - accuracy: 0.9526 - val_loss: 0.4436 - val_accuracy: 0.8938\n",
      "Epoch 104/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1306 - accuracy: 0.9618\n",
      "Epoch 104: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 106s 5s/step - loss: 0.1306 - accuracy: 0.9618 - val_loss: 0.5528 - val_accuracy: 0.8875\n",
      "Epoch 105/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1396 - accuracy: 0.9587\n",
      "Epoch 105: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 111s 5s/step - loss: 0.1396 - accuracy: 0.9587 - val_loss: 0.4276 - val_accuracy: 0.9187\n",
      "Epoch 106/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1397 - accuracy: 0.9541\n",
      "Epoch 106: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 102s 5s/step - loss: 0.1397 - accuracy: 0.9541 - val_loss: 0.7422 - val_accuracy: 0.8625\n",
      "Epoch 107/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1480 - accuracy: 0.9511\n",
      "Epoch 107: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 103s 5s/step - loss: 0.1480 - accuracy: 0.9511 - val_loss: 0.4830 - val_accuracy: 0.8750\n",
      "Epoch 108/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1952 - accuracy: 0.9251\n",
      "Epoch 108: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 95s 5s/step - loss: 0.1952 - accuracy: 0.9251 - val_loss: 0.7094 - val_accuracy: 0.8375\n",
      "Epoch 109/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1773 - accuracy: 0.9434\n",
      "Epoch 109: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 101s 5s/step - loss: 0.1773 - accuracy: 0.9434 - val_loss: 0.6517 - val_accuracy: 0.8562\n",
      "Epoch 110/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1568 - accuracy: 0.9450\n",
      "Epoch 110: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 100s 5s/step - loss: 0.1568 - accuracy: 0.9450 - val_loss: 0.5941 - val_accuracy: 0.8750\n",
      "Epoch 111/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1539 - accuracy: 0.9479\n",
      "Epoch 111: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 105s 5s/step - loss: 0.1539 - accuracy: 0.9479 - val_loss: 0.4746 - val_accuracy: 0.8938\n",
      "Epoch 112/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0851 - accuracy: 0.9709\n",
      "Epoch 112: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 104s 5s/step - loss: 0.0851 - accuracy: 0.9709 - val_loss: 0.5243 - val_accuracy: 0.8875\n",
      "Epoch 113/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0884 - accuracy: 0.9679\n",
      "Epoch 113: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 103s 5s/step - loss: 0.0884 - accuracy: 0.9679 - val_loss: 0.4675 - val_accuracy: 0.9000\n",
      "Epoch 114/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1208 - accuracy: 0.9480\n",
      "Epoch 114: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 104s 5s/step - loss: 0.1208 - accuracy: 0.9480 - val_loss: 0.3746 - val_accuracy: 0.9125\n",
      "Epoch 115/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0825 - accuracy: 0.9755\n",
      "Epoch 115: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 112s 5s/step - loss: 0.0825 - accuracy: 0.9755 - val_loss: 0.5229 - val_accuracy: 0.9187\n",
      "Epoch 116/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9740\n",
      "Epoch 116: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 115s 5s/step - loss: 0.0672 - accuracy: 0.9740 - val_loss: 0.6149 - val_accuracy: 0.8687\n",
      "Epoch 117/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1446 - accuracy: 0.9618\n",
      "Epoch 117: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 104s 5s/step - loss: 0.1446 - accuracy: 0.9618 - val_loss: 0.6160 - val_accuracy: 0.9000\n",
      "Epoch 118/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1157 - accuracy: 0.9526\n",
      "Epoch 118: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 100s 5s/step - loss: 0.1157 - accuracy: 0.9526 - val_loss: 0.6759 - val_accuracy: 0.8375\n",
      "Epoch 119/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1207 - accuracy: 0.9664\n",
      "Epoch 119: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 107s 5s/step - loss: 0.1207 - accuracy: 0.9664 - val_loss: 0.5648 - val_accuracy: 0.9125\n",
      "Epoch 120/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1324 - accuracy: 0.9648\n",
      "Epoch 120: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 113s 5s/step - loss: 0.1324 - accuracy: 0.9648 - val_loss: 0.6414 - val_accuracy: 0.8813\n",
      "Epoch 121/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9755\n",
      "Epoch 121: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 103s 5s/step - loss: 0.0692 - accuracy: 0.9755 - val_loss: 0.7125 - val_accuracy: 0.8687\n",
      "Epoch 122/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0901 - accuracy: 0.9709\n",
      "Epoch 122: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 101s 5s/step - loss: 0.0901 - accuracy: 0.9709 - val_loss: 0.7286 - val_accuracy: 0.8562\n",
      "Epoch 123/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9771\n",
      "Epoch 123: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 102s 5s/step - loss: 0.0788 - accuracy: 0.9771 - val_loss: 0.5403 - val_accuracy: 0.8750\n",
      "Epoch 124/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0841 - accuracy: 0.9755\n",
      "Epoch 124: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 112s 5s/step - loss: 0.0841 - accuracy: 0.9755 - val_loss: 0.5945 - val_accuracy: 0.8687\n",
      "Epoch 125/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9725\n",
      "Epoch 125: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 104s 5s/step - loss: 0.0775 - accuracy: 0.9725 - val_loss: 0.5345 - val_accuracy: 0.8562\n",
      "Epoch 126/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0992 - accuracy: 0.9679\n",
      "Epoch 126: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.0992 - accuracy: 0.9679 - val_loss: 0.5451 - val_accuracy: 0.8875\n",
      "Epoch 127/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1564 - accuracy: 0.9557\n",
      "Epoch 127: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 100s 5s/step - loss: 0.1564 - accuracy: 0.9557 - val_loss: 0.7329 - val_accuracy: 0.8625\n",
      "Epoch 128/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1061 - accuracy: 0.9602\n",
      "Epoch 128: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 118s 6s/step - loss: 0.1061 - accuracy: 0.9602 - val_loss: 0.7119 - val_accuracy: 0.8562\n",
      "Epoch 129/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0563 - accuracy: 0.9801\n",
      "Epoch 129: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 105s 5s/step - loss: 0.0563 - accuracy: 0.9801 - val_loss: 0.6084 - val_accuracy: 0.8562\n",
      "Epoch 130/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1008 - accuracy: 0.9725\n",
      "Epoch 130: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 114s 5s/step - loss: 0.1008 - accuracy: 0.9725 - val_loss: 0.8346 - val_accuracy: 0.8438\n",
      "Epoch 131/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9725\n",
      "Epoch 131: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 111s 5s/step - loss: 0.0753 - accuracy: 0.9725 - val_loss: 0.7812 - val_accuracy: 0.8500\n",
      "Epoch 132/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1016 - accuracy: 0.9572\n",
      "Epoch 132: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 109s 5s/step - loss: 0.1016 - accuracy: 0.9572 - val_loss: 0.7009 - val_accuracy: 0.8562\n",
      "Epoch 133/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9747\n",
      "Epoch 133: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 117s 6s/step - loss: 0.0668 - accuracy: 0.9747 - val_loss: 0.6529 - val_accuracy: 0.8562\n",
      "Epoch 134/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0957 - accuracy: 0.9694\n",
      "Epoch 134: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 110s 5s/step - loss: 0.0957 - accuracy: 0.9694 - val_loss: 0.7607 - val_accuracy: 0.8750\n",
      "Epoch 135/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0969 - accuracy: 0.9709\n",
      "Epoch 135: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 109s 5s/step - loss: 0.0969 - accuracy: 0.9709 - val_loss: 0.5866 - val_accuracy: 0.8750\n",
      "Epoch 136/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1062 - accuracy: 0.9679\n",
      "Epoch 136: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 107s 5s/step - loss: 0.1062 - accuracy: 0.9679 - val_loss: 0.5003 - val_accuracy: 0.8750\n",
      "Epoch 137/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1058 - accuracy: 0.9725\n",
      "Epoch 137: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 102s 5s/step - loss: 0.1058 - accuracy: 0.9725 - val_loss: 0.5210 - val_accuracy: 0.9000\n",
      "Epoch 138/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1226 - accuracy: 0.9648\n",
      "Epoch 138: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 104s 5s/step - loss: 0.1226 - accuracy: 0.9648 - val_loss: 0.6461 - val_accuracy: 0.8938\n",
      "Epoch 139/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0888 - accuracy: 0.9709\n",
      "Epoch 139: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 100s 5s/step - loss: 0.0888 - accuracy: 0.9709 - val_loss: 0.5913 - val_accuracy: 0.8562\n",
      "Epoch 140/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9801\n",
      "Epoch 140: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 100s 5s/step - loss: 0.0684 - accuracy: 0.9801 - val_loss: 0.6298 - val_accuracy: 0.8687\n",
      "Epoch 141/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1358 - accuracy: 0.9648\n",
      "Epoch 141: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 100s 5s/step - loss: 0.1358 - accuracy: 0.9648 - val_loss: 0.4380 - val_accuracy: 0.8750\n",
      "Epoch 142/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0606 - accuracy: 0.9755\n",
      "Epoch 142: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 103s 5s/step - loss: 0.0606 - accuracy: 0.9755 - val_loss: 0.5555 - val_accuracy: 0.8813\n",
      "Epoch 143/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1087 - accuracy: 0.9694\n",
      "Epoch 143: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 102s 5s/step - loss: 0.1087 - accuracy: 0.9694 - val_loss: 0.6552 - val_accuracy: 0.8813\n",
      "Epoch 144/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0490 - accuracy: 0.9786\n",
      "Epoch 144: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 103s 5s/step - loss: 0.0490 - accuracy: 0.9786 - val_loss: 0.6296 - val_accuracy: 0.8813\n",
      "Epoch 145/1000\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.9771\n",
      "Epoch 145: val_loss did not improve from 0.34322\n",
      "21/21 [==============================] - 103s 5s/step - loss: 0.0658 - accuracy: 0.9771 - val_loss: 0.6390 - val_accuracy: 0.8813\n"
     ]
    }
   ],
   "source": [
    "train_step = num_train//BATCH_SIZE\n",
    "val_step = num_val//BATCH_SIZE\n",
    "\n",
    "filepath = \"./model/{epoch:03d}-{val_loss:.4f}.h5\"\n",
    "check_point = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=filepath,\n",
    "    save_best_only = True,\n",
    "    verbose = True\n",
    ")\n",
    "early_stop_point = keras.callbacks.EarlyStopping(patience=100)\n",
    "\n",
    "hist = finetune_model.fit(\n",
    "    train_ds, epochs=1000,\n",
    "    steps_per_epoch=train_step,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps=val_step,\n",
    "    callbacks = [check_point, early_stop_point]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38d9dda6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history[\"loss\"], c='b', label=\"train_loss\")\n",
    "plt.plot(hist.history[\"val_loss\"], c='r', label=\"val loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7b6829",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
